{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake.json')\n",
    "df_cpc = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/df_patentsview_patent_cpc_grouped_cleantech.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df and df_cpc on 'patent_id' keep 'claim_fulltext' and 'keywords_yake' for df, keep 'patent_title' and 'cpc' for df_cpc\n",
    "df_merged = df.merge(df_cpc, on='patent_id', how='left')\n",
    "df_merged = df_merged[['patent_id', 'claim_fulltext', 'keywords_yake', 'patent_title', 'cpc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_merged['keywords_yake'] to list\n",
    "df_merged['keywords_yake'] = df_merged['keywords_yake'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sets for unique keyword tracking\n",
    "cpc_subclass_keywords_set = set()\n",
    "cpc_subgroup_keywords_set = set()\n",
    "\n",
    "# Iterate through cpc dictionary column of df_merged\n",
    "for cpc_dict in df_merged['cpc']:\n",
    "    # Iterate over nested cpc dictionary of classifications for each patent\n",
    "    for outer_key, outer_value in cpc_dict.items():\n",
    "        # Iterate over entries in each classification\n",
    "        for inner_key, inner_value in outer_value.items():\n",
    "            if inner_key == 'cpc_subclass':\n",
    "                cpc_subclass_keywords_set.add(inner_value)\n",
    "            elif inner_key == 'cpc_subgroup':\n",
    "                cpc_subgroup_keywords_set.add(inner_value)\n",
    "\n",
    "# Convert sets to dataframes\n",
    "df_cpc_subclass_keywords = pd.DataFrame(list(cpc_subclass_keywords_set), columns=['cpc_subclass'])\n",
    "df_cpc_subgroup_keywords = pd.DataFrame(list(cpc_subgroup_keywords_set), columns=['cpc_subgroup'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to hold keywords\n",
    "subclass_keywords_dict = {subclass: [] for subclass in df_cpc_subclass_keywords['cpc_subclass']}\n",
    "subgroup_keywords_dict = {subgroup: [] for subgroup in df_cpc_subgroup_keywords['cpc_subgroup']}\n",
    "\n",
    "# Iterate over DataFrame rows\n",
    "for index, row in df_merged.iterrows():\n",
    "    keywords_yake = row['keywords_yake']\n",
    "    # Assuming 'cpc' column has a dictionary structure\n",
    "    for outer_key, outer_value in row['cpc'].items():\n",
    "        for inner_key, inner_value in outer_value.items():\n",
    "            # Check and append keywords to the corresponding subclass or subgroup\n",
    "            if inner_key == 'cpc_subclass' and inner_value in subclass_keywords_dict:\n",
    "                subclass_keywords_dict[inner_value].append(keywords_yake)\n",
    "            elif inner_key == 'cpc_subgroup' and inner_value in subgroup_keywords_dict:\n",
    "                subgroup_keywords_dict[inner_value].append(keywords_yake)\n",
    "\n",
    "# Convert the dictionaries back to DataFrame\n",
    "df_cpc_subclass_keywords['keywords_yake'] = df_cpc_subclass_keywords['cpc_subclass'].map(subclass_keywords_dict)\n",
    "df_cpc_subgroup_keywords['keywords_yake'] = df_cpc_subgroup_keywords['cpc_subgroup'].map(subgroup_keywords_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order alphabetically\n",
    "df_cpc_subclass_keywords.sort_values(by=['cpc_subclass'], inplace=True)\n",
    "df_cpc_subgroup_keywords.sort_values(by=['cpc_subgroup'], inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df_cpc_subclass_keywords.reset_index(drop=True, inplace=True)\n",
    "df_cpc_subgroup_keywords.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to flatten the list of lists\n",
    "def flatten_list_of_lists(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "# Apply the function to the 'keywords_yake' column of each DataFrame\n",
    "df_cpc_subclass_keywords['keywords_yake'] = df_cpc_subclass_keywords['keywords_yake'].apply(flatten_list_of_lists)\n",
    "df_cpc_subgroup_keywords['keywords_yake'] = df_cpc_subgroup_keywords['keywords_yake'].apply(flatten_list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to process the keywords\n",
    "def process_keywords(keywords):\n",
    "    # Lemmatize the keywords\n",
    "    lemmatized_keywords = [lemmatizer.lemmatize(word) for word in keywords]\n",
    "    # Calculate keyword frequencies\n",
    "    keyword_frequencies = Counter(lemmatized_keywords)\n",
    "    # Sort keywords by frequency in descending order, and remove duplicates\n",
    "    sorted_keywords = [item[0] for item in keyword_frequencies.most_common()]\n",
    "    return sorted_keywords\n",
    "\n",
    "# Apply the function to the 'keywords_yake' column and store the result in the 'keywords_yake_desc' column\n",
    "df_cpc_subclass_keywords['keywords_yake_desc'] = df_cpc_subclass_keywords['keywords_yake'].apply(process_keywords)\n",
    "df_cpc_subgroup_keywords['keywords_yake_desc'] = df_cpc_subgroup_keywords['keywords_yake'].apply(process_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter out keywords with less than two words\n",
    "def filter_keywords(keywords):\n",
    "    return [keyword for keyword in keywords if len(keyword.split()) >= 2]\n",
    "\n",
    "# Apply the function to the 'keywords_yake_desc' column of each DataFrame\n",
    "df_cpc_subclass_keywords['keywords_yake_desc'] = df_cpc_subclass_keywords['keywords_yake_desc'].apply(filter_keywords)\n",
    "df_cpc_subgroup_keywords['keywords_yake_desc'] = df_cpc_subgroup_keywords['keywords_yake_desc'].apply(filter_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter out keywords containing 'claim'\n",
    "def filter_keywords(keywords):\n",
    "    return [keyword for keyword in keywords if 'claim' not in keyword.lower()]\n",
    "\n",
    "# Apply the function to the 'keywords_yake_desc' column of each DataFrame\n",
    "df_cpc_subclass_keywords['keywords_yake_desc'] = df_cpc_subclass_keywords['keywords_yake_desc'].apply(filter_keywords)\n",
    "df_cpc_subgroup_keywords['keywords_yake_desc'] = df_cpc_subgroup_keywords['keywords_yake_desc'].apply(filter_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cross_row_duplicates(df, column_name):\n",
    "    seen_keywords = set()  # Set to keep track of keywords that have been seen\n",
    "\n",
    "    def filter_duplicates(keywords_list):\n",
    "        nonlocal seen_keywords  # Allow access to the outer scope variable\n",
    "        new_keywords_list = []  # List to keep the filtered keywords\n",
    "        for keyword in keywords_list:\n",
    "            # Only add keyword to new list if it hasn't been seen before\n",
    "            if keyword not in seen_keywords:\n",
    "                new_keywords_list.append(keyword)\n",
    "                seen_keywords.add(keyword)  # Mark keyword as seen\n",
    "        return new_keywords_list\n",
    "\n",
    "    # Apply the function to filter out duplicates from each row\n",
    "    df[column_name] = df[column_name].apply(filter_duplicates)\n",
    "\n",
    "# Apply the function to your DataFrames\n",
    "filter_cross_row_duplicates(df_cpc_subclass_keywords, 'keywords_yake_desc')\n",
    "filter_cross_row_duplicates(df_cpc_subgroup_keywords, 'keywords_yake_desc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a fourth column keywords_yake_desc_5000 that contains the first 5000 keywords of each row\n",
    "df_cpc_subclass_keywords['keywords_yake_desc_5000'] = df_cpc_subclass_keywords['keywords_yake_desc'].apply(lambda x: x[:5000])\n",
    "# Make a fourth column keywords_yake_desc_1000 that contains the first 1000 keywords of each row\n",
    "df_cpc_subgroup_keywords['keywords_yake_desc_1000'] = df_cpc_subgroup_keywords['keywords_yake_desc'].apply(lambda x: x[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrames to JSON files\n",
    "df_cpc_subclass_keywords.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/df_cpc_subclass_keywords_yake.json', orient='records')\n",
    "df_cpc_subgroup_keywords.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/df_cpc_subgroup_keywords_yake.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
