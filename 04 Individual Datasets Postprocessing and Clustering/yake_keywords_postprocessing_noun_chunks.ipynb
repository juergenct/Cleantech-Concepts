{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# df_yake_claims_ep = pd.read_json('/mnt/hdd01/PATSTAT Working Directory/PATSTAT/cleantech_epo_text_data_pivot_cleaned_yake_noun_chunks.json')\n",
    "df_yake_claims_ep = pd.read_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/df_epo_non_cleantech_text_data_pivot_claims_cleaned_yake_noun_chunks.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_strings(s):\n",
    "    # Check if the string starts with [' and ends with ']\n",
    "    if s.startswith(\"['\") and s.endswith(\"']\"):\n",
    "        # Use a regular expression to find all sequences of characters enclosed in single or double quotes\n",
    "        return re.findall(r\"['\\\"]([^'\\\"]*)['\\\"]\", s)\n",
    "    else:\n",
    "        # Split the string by commas\n",
    "        return s.split(', ')\n",
    "    \n",
    "# Apply parse_strings function to 'cpc_class_symbol' column\n",
    "df_yake_claims_ep['cpc_class_symbol'] = df_yake_claims_ep['cpc_class_symbol'].progress_apply(parse_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_ep = []\n",
    "yake_conf_score_list = []\n",
    "publn_nr_list = []\n",
    "cpc_symbol_list = [] # - Not considered for Non Cleantech Patents\n",
    "min_yake_conf = 0.1 # - Currently not used\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_claims_ep.iterrows()):\n",
    "    # Check if 'keywords_yake_claims' column is not a list\n",
    "    if not isinstance(row['keywords_yake_claim_noun_chunk'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake_claims' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_claim_noun_chunk']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake_claims' column and append to keywords_list_ep, consider only top 10 keywords\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_claim_noun_chunk'][:10]:\n",
    "            # if keyword[1] <= min_yake_conf:\n",
    "            keywords_list_ep.append(keyword[0].lower())\n",
    "            yake_conf_score_list.append(keyword[1])\n",
    "            publn_nr_list.append(row['publn_nr'])\n",
    "            # cpc_symbol_list.append(row['cpc_class_symbol'])\n",
    "\n",
    "# Create new dataframe\n",
    "df_keywords_list_ep = pd.DataFrame({\n",
    "    'keyword_yake': keywords_list_ep,\n",
    "    'yake_conf_score': yake_conf_score_list,\n",
    "    'publn_nr': publn_nr_list,\n",
    "    # 'cpc_class_symbol': cpc_symbol_list,\n",
    "    'abs_frequency': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-alphanumeric keywords\n",
    "df_keywords_list_ep = df_keywords_list_ep[\n",
    "    df_keywords_list_ep['keyword_yake'].progress_apply(lambda x: all(word.isalnum() for word in x.split()))\n",
    "]\n",
    "\n",
    "# Filter out all keywords shorter than 3 characters\n",
    "df_keywords_list_ep = df_keywords_list_ep[\n",
    "    df_keywords_list_ep['keyword_yake'].progress_apply(lambda x: len(x) > 2)\n",
    "]\n",
    "\n",
    "# Define a function to check if a string is an abbreviation\n",
    "def is_abbreviation(keyword):\n",
    "    # Regular expression to identify abbreviations (typically all uppercase and periods)\n",
    "    # and check for all-uppercase abbreviations with 3 or fewer characters\n",
    "    pattern = re.compile(r'\\b(?:[A-Z]{1,}\\.){2,}\\b|\\b[A-Z]{1,3}\\b')\n",
    "    return pattern.match(keyword) is not None\n",
    "\n",
    "# Apply the function to filter out abbreviations\n",
    "df_keywords_list_ep = df_keywords_list_ep[\n",
    "    df_keywords_list_ep['keyword_yake'].progress_apply(lambda x: not is_abbreviation(x))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize keywords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_keywords(keyword):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in keyword.split()])\n",
    "\n",
    "df_keywords_list_ep['keyword_yake_lemma'] = df_keywords_list_ep['keyword_yake'].progress_apply(lemmatize_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to remove keywords that are only one stopword or start/end with a stopword\n",
    "def remove_stopwords(keyword):\n",
    "    words = keyword.split()\n",
    "    \n",
    "    # If the keyword is a single stopword, remove it\n",
    "    if len(words) == 1 and words[0] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    # If the keyword starts or ends with a stopword, remove line\n",
    "    if words[0] in stopwords:\n",
    "        return ''\n",
    "    if words and words[-1] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the function to remove stopwords\n",
    "df_keywords_list_ep['keyword_yake_lemma'] = df_keywords_list_ep['keyword_yake_lemma'].progress_apply(remove_stopwords)\n",
    "\n",
    "# Remove empty keywords\n",
    "df_keywords_list_ep = df_keywords_list_ep[\n",
    "    df_keywords_list_ep['keyword_yake_lemma'].progress_apply(lambda x: len(x) > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_keywords_list_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate df_keywords_list_ep by 'keyword'\n",
    "df_keywords_list_ep_agg = df_keywords_list_ep.groupby(['keyword_yake_lemma']).agg({\n",
    "    'yake_conf_score': 'mean',\n",
    "    'publn_nr': list,\n",
    "    # 'cpc_class_symbol': list,\n",
    "    'abs_frequency': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten nested lists in 'cpc_class_symbol' column\n",
    "# df_keywords_list_ep_agg['cpc_class_symbol'] = df_keywords_list_ep_agg['cpc_class_symbol'].progress_apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_keywords_list_ep_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_keywords_list_ep_agg.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/epo_yake_keywords_list_noun_chunks.json', orient='records')\n",
    "df_keywords_list_ep_agg.to_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/epo_yake_keywords_list_non_cleantech_noun_chunks.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# df_yake_claims_uspto = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/g_patent_claims_cleantech_yake_noun_chunks.json')\n",
    "df_yake_claims_uspto = pd.read_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/g_uspto_non_cleantech_claims_fulltext_yake_noun_chunks.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_uspto = []\n",
    "yake_conf_score_list = []\n",
    "patent_id_list = []\n",
    "min_yake_conf = 0.1 # - Currently not used\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_claims_uspto.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['keywords_yake_claim_noun_chunk'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_claim_noun_chunk']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append to keywords_list_uspto\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_claim_noun_chunk'][:10]:\n",
    "            # if keyword[1] <= min_yake_conf:\n",
    "            keywords_list_uspto.append(keyword[0].lower())\n",
    "            yake_conf_score_list.append(keyword[1])\n",
    "            patent_id_list.append(row['patent_id'])\n",
    "\n",
    "# Create new dataframe\n",
    "df_keywords_list_uspto = pd.DataFrame({\n",
    "    'keyword_yake': keywords_list_uspto,\n",
    "    'yake_conf_score': yake_conf_score_list,\n",
    "    'patent_id': patent_id_list,\n",
    "    'abs_frequency': 1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-alphanumeric keywords\n",
    "df_keywords_list_uspto = df_keywords_list_uspto[\n",
    "    df_keywords_list_uspto['keyword_yake'].progress_apply(lambda x: all(word.isalnum() for word in x.split()))\n",
    "]\n",
    "\n",
    "# Filter out all keywords shorter than 3 characters\n",
    "df_keywords_list_uspto = df_keywords_list_uspto[\n",
    "    df_keywords_list_uspto['keyword_yake'].progress_apply(lambda x: len(x) > 2)\n",
    "]\n",
    "\n",
    "# Define a function to check if a string is an abbreviation\n",
    "def is_abbreviation(keyword):\n",
    "    # Regular expression to identify abbreviations (typically all uppercase and periods)\n",
    "    # and check for all-uppercase abbreviations with 3 or fewer characters\n",
    "    pattern = re.compile(r'\\b(?:[A-Z]{1,}\\.){2,}\\b|\\b[A-Z]{1,3}\\b')\n",
    "    return pattern.match(keyword) is not None\n",
    "\n",
    "# Apply the function to filter out abbreviations\n",
    "df_keywords_list_uspto = df_keywords_list_uspto[\n",
    "    df_keywords_list_uspto['keyword_yake'].progress_apply(lambda x: not is_abbreviation(x))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Patents to CPC Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatentsView - Merge with CPC Classification\n",
    "df_cpc_uspto = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/df_patentsview_patent_cpc_grouped_cleantech.json')\n",
    "# Extract 'cpc_group' into a new column\n",
    "df_cpc_uspto['cpc_group'] = df_cpc_uspto['cpc'].progress_apply(\n",
    "    lambda x: [entry['cpc_group'] for entry in x.values() if 'cpc_group' in entry]\n",
    ")\n",
    "\n",
    "# Remove duplicates from 'cpc_group_list'\n",
    "df_cpc_uspto['cpc_group'] = df_cpc_uspto['cpc_group'].progress_apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_keywords_list_uspto with df_cpc_uspto\n",
    "df_keywords_list_uspto = pd.merge(\n",
    "    df_keywords_list_uspto,\n",
    "    df_cpc_uspto[['patent_id', 'cpc_group']],\n",
    "    how='left',\n",
    "    on='patent_id'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize keywords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_keywords(keyword):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in keyword.split()])\n",
    "\n",
    "df_keywords_list_uspto['keyword_yake_lemma'] = df_keywords_list_uspto['keyword_yake'].progress_apply(lemmatize_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to remove keywords that are only one stopword or start/end with a stopword\n",
    "def remove_stopwords(keyword):\n",
    "    words = keyword.split()\n",
    "    \n",
    "    # If the keyword is a single stopword, remove it\n",
    "    if len(words) == 1 and words[0] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    # If the keyword starts or ends with a stopword, remove line\n",
    "    if words[0] in stopwords:\n",
    "        return ''\n",
    "    if words and words[-1] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the function to remove stopwords\n",
    "df_keywords_list_uspto['keyword_yake_lemma'] = df_keywords_list_uspto['keyword_yake_lemma'].progress_apply(remove_stopwords)\n",
    "\n",
    "# Remove empty keywords\n",
    "df_keywords_list_uspto = df_keywords_list_uspto[\n",
    "    df_keywords_list_uspto['keyword_yake_lemma'].progress_apply(lambda x: len(x) > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate df_keywords_list_ep by 'keyword'\n",
    "df_keywords_list_uspto_agg = df_keywords_list_uspto.groupby(['keyword_yake_lemma']).agg({\n",
    "    'yake_conf_score': 'mean',\n",
    "    'patent_id': list,\n",
    "    # 'cpc_group': list,\n",
    "    'abs_frequency': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten nested lists in 'cpc_group' column\n",
    "# df_keywords_list_uspto_agg['cpc_group'] = df_keywords_list_uspto_agg['cpc_group'].progress_apply(lambda x: [item for sublist in x for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_keywords_list_uspto_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_keywords_list_uspto_agg.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/uspto_yake_keywords_list_noun_chunks.json', orient='records')\n",
    "df_keywords_list_uspto_agg.to_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/uspto_yake_keywords_list_non_cleantech_noun_chunks.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliance on Science - USPTO and EPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_rel_on_science_uspto = pd.read_json('/mnt/hdd01/patentsview/Reliance on Science - Cleantech Patents/df_oaid_Cleantech_y02_individual_works_lang_detect_yake_noun_chunks.json', dtype={'patent_id': str, 'oaid': str})\n",
    "df_rel_on_science_ep = pd.read_json('/mnt/hdd01/PATSTAT Working Directory/Reliance on Science/cleantech_epo_rel_on_science_abstract_lang_detect_yake_noun_chunks.json', dtype={'publn_nr': str, 'oaid': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_on_science = pd.read_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/df_oaids_non_cleantech_lang_detect_yake_noun_chunks.json', dtype={'oaid': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes, reset index and drop duplicates\n",
    "df_rel_on_science = pd.concat([df_rel_on_science_uspto, df_rel_on_science_ep], ignore_index=True)\n",
    "df_rel_on_science = df_rel_on_science.drop_duplicates(subset=['oaid'], keep='first').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rel_on_science.to_json('/mnt/hdd01/patentsview/Reliance on Science - Cleantech Patents/df_oaid_cleantech_lang_detect_yake_noun_chunks.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns keywords_yake_claim and keywords_yake_claim_noun_chunk to keywords_yake_abstract and keywords_yake_abstract_noun_chunk\n",
    "df_rel_on_science = df_rel_on_science.rename(columns={'keywords_yake_claim': 'keywords_yake_abstract', 'keywords_yake_claim_noun_chunk': 'keywords_yake_abstract_noun_chunk'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_rel = []\n",
    "yake_conf_score_list = []\n",
    "oaid_list = []\n",
    "# publn_nr_list = []\n",
    "# patent_id_list = []\n",
    "# patent_list = []\n",
    "min_yake_conf = 0.1 # - Currently not used\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_rel_on_science.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['keywords_yake_abstract_noun_chunk'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_abstract_noun_chunk']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append to keywords_list_rel\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_abstract_noun_chunk'][:10]:\n",
    "            # if keyword[1] <= min_yake_conf:\n",
    "            keywords_list_rel.append(keyword[0].lower())\n",
    "            yake_conf_score_list.append(keyword[1])\n",
    "            oaid_list.append(row['oaid'])\n",
    "            # patent_list.append(row['patent'])\n",
    "            # publn_nr_list.append(row['publn_nr'])\n",
    "            # patent_id_list.append(row['patent_id'])\n",
    "\n",
    "# Create new dataframe\n",
    "df_keywords_list_rel = pd.DataFrame({\n",
    "    'keyword_yake': keywords_list_rel,\n",
    "    'yake_conf_score': yake_conf_score_list,\n",
    "    'oaid': oaid_list,\n",
    "    'abs_frequency': 1,\n",
    "    # 'publn_nr': publn_nr_list,\n",
    "    # 'patent_id': patent_id_list\n",
    "    # 'patent': patent_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-alphanumeric keywords\n",
    "df_keywords_list_rel = df_keywords_list_rel[\n",
    "    df_keywords_list_rel['keyword_yake'].progress_apply(lambda x: all(word.isalnum() for word in x.split()))\n",
    "]\n",
    "\n",
    "# Filter out all keywords shorter than 3 characters\n",
    "df_keywords_list_rel = df_keywords_list_rel[\n",
    "    df_keywords_list_rel['keyword_yake'].progress_apply(lambda x: len(x) > 2)\n",
    "]\n",
    "\n",
    "# Define a function to check if a string is an abbreviation\n",
    "def is_abbreviation(keyword):\n",
    "    # Regular expression to identify abbreviations (typically all uppercase and periods)\n",
    "    # and check for all-uppercase abbreviations with 3 or fewer characters\n",
    "    pattern = re.compile(r'\\b(?:[A-Z]{1,}\\.){2,}\\b|\\b[A-Z]{1,3}\\b')\n",
    "    return pattern.match(keyword) is not None\n",
    "\n",
    "# Apply the function to filter out abbreviations\n",
    "df_keywords_list_rel = df_keywords_list_rel[\n",
    "    df_keywords_list_rel['keyword_yake'].progress_apply(lambda x: not is_abbreviation(x))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize keywords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_keywords(keyword):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in keyword.split()])\n",
    "\n",
    "df_keywords_list_rel['keyword_yake_lemma'] = df_keywords_list_rel['keyword_yake'].progress_apply(lemmatize_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to remove keywords that are only one stopword or start/end with a stopword\n",
    "def remove_stopwords(keyword):\n",
    "    words = keyword.split()\n",
    "    \n",
    "    # If the keyword is a single stopword, remove it\n",
    "    if len(words) == 1 and words[0] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    # If the keyword starts or ends with a stopword, remove line\n",
    "    if words[0] in stopwords:\n",
    "        return ''\n",
    "    if words and words[-1] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the function to remove stopwords\n",
    "df_keywords_list_rel['keyword_yake_lemma'] = df_keywords_list_rel['keyword_yake_lemma'].progress_apply(remove_stopwords)\n",
    "\n",
    "# Remove empty keywords\n",
    "df_keywords_list_rel = df_keywords_list_rel[\n",
    "    df_keywords_list_rel['keyword_yake_lemma'].progress_apply(lambda x: len(x) > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'publn_nr' column to string\n",
    "# df_keywords_list_rel['publn_nr'] = df_keywords_list_rel['publn_nr'].progress_apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate df_keywords_list_rel by 'keyword_yake_lemma'\n",
    "df_keywords_list_rel_agg = df_keywords_list_rel.groupby(['keyword_yake_lemma']).agg({\n",
    "    'yake_conf_score': 'mean',\n",
    "    'oaid': list,\n",
    "    # 'publn_nr': list,\n",
    "    # 'patent': list,\n",
    "    # 'patent_id': list,\n",
    "    'abs_frequency': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Delete all nan list entries in columns 'publn_nr' and 'patent_id'\n",
    "# df_keywords_list_rel_agg['publn_nr'] = df_keywords_list_rel_agg['publn_nr'].progress_apply(lambda x: [item for item in x if str(item) != 'nan'])\n",
    "# df_keywords_list_rel_agg['patent_id'] = df_keywords_list_rel_agg['patent_id'].progress_apply(lambda x: [item for item in x if str(item) != 'nan'])\n",
    "# df_keywords_list_rel_agg['patent'] = df_keywords_list_rel_agg['patent'].progress_apply(lambda x: [item for item in x if str(item) != 'nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_keywords_list_rel_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_keywords_list_rel_agg.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/rel_on_science_yake_keywords_list_noun_chunks.json', orient='records')\n",
    "df_keywords_list_rel_agg.to_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/rel_on_science_yake_keywords_list_non_cleantech_noun_chunks.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPC Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpc_classification = pd.read_json('/mnt/hdd01/patentsview/CPC Classification/df_keyword_y02_classification_noun_chunking.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list_cpc = []\n",
    "yake_conf_score_list = []\n",
    "cpc_symbol_list = []\n",
    "min_yake_conf = 0.1 # - Currently not used\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_cpc_classification.iterrows()):\n",
    "    # Check if 'keywords_yake_claims' column is not a list\n",
    "    if not isinstance(row['keywords_yake_title_noun_chunk'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake_claims' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_title_noun_chunk']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake_claims' column and append to keywords_list_ep, consider only top 10 keywords\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_title_noun_chunk'][:10]:\n",
    "            # if keyword[1] <= min_yake_conf:\n",
    "            keywords_list_cpc.append(keyword[0].lower())\n",
    "            yake_conf_score_list.append(keyword[1])\n",
    "            cpc_symbol_list.append(row['cpc_classification'])\n",
    "\n",
    "# Create new dataframe\n",
    "df_keywords_list_cpc = pd.DataFrame({\n",
    "    'keyword_yake': keywords_list_cpc,\n",
    "    'yake_conf_score': yake_conf_score_list,\n",
    "    'cpc_class_symbol': cpc_symbol_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-alphanumeric keywords\n",
    "df_keywords_list_cpc = df_keywords_list_cpc[\n",
    "    df_keywords_list_cpc['keyword_yake'].progress_apply(lambda x: all(word.isalnum() for word in x.split()))\n",
    "]\n",
    "\n",
    "# Filter out all keywords shorter than 3 characters\n",
    "df_keywords_list_cpc = df_keywords_list_cpc[\n",
    "    df_keywords_list_cpc['keyword_yake'].progress_apply(lambda x: len(x) > 2)\n",
    "]\n",
    "\n",
    "# Define a function to check if a string is an abbreviation\n",
    "def is_abbreviation(keyword):\n",
    "    # Regular expression to identify abbreviations (typically all uppercase and periods)\n",
    "    # and check for all-uppercase abbreviations with 3 or fewer characters\n",
    "    pattern = re.compile(r'\\b(?:[A-Z]{1,}\\.){2,}\\b|\\b[A-Z]{1,3}\\b')\n",
    "    return pattern.match(keyword) is not None\n",
    "\n",
    "# Apply the function to filter out abbreviations\n",
    "df_keywords_list_cpc = df_keywords_list_cpc[\n",
    "    df_keywords_list_cpc['keyword_yake'].progress_apply(lambda x: not is_abbreviation(x))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize keywords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_keywords(keyword):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in keyword.split()])\n",
    "\n",
    "df_keywords_list_cpc['keyword_yake_lemma'] = df_keywords_list_cpc['keyword_yake'].progress_apply(lemmatize_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Function to remove keywords that are only one stopword or start/end with a stopword\n",
    "def remove_stopwords(keyword):\n",
    "    words = keyword.split()\n",
    "    \n",
    "    # If the keyword is a single stopword, remove it\n",
    "    if len(words) == 1 and words[0] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    # If the keyword starts or ends with a stopword, remove line\n",
    "    if words[0] in stopwords:\n",
    "        return ''\n",
    "    if words and words[-1] in stopwords:\n",
    "        return ''\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the function to remove stopwords\n",
    "df_keywords_list_cpc['keyword_yake_lemma'] = df_keywords_list_cpc['keyword_yake_lemma'].progress_apply(remove_stopwords)\n",
    "\n",
    "# Remove empty keywords\n",
    "df_keywords_list_cpc = df_keywords_list_cpc[\n",
    "    df_keywords_list_cpc['keyword_yake_lemma'].progress_apply(lambda x: len(x) > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_keywords_list_cpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate df_keywords_list_cpc by 'keyword'\n",
    "df_keywords_list_cpc_agg = df_keywords_list_cpc.groupby(['keyword_yake_lemma']).agg({\n",
    "    'yake_conf_score': 'mean',\n",
    "    'cpc_class_symbol': list,\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_keywords_list_cpc_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_cpc_agg.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/cpc_yake_keywords_list_noun_chunks.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_cpc_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate copy of df_claims_keywords_list\n",
    "df_keywords_list_cpc_embeddings = df_keywords_list_cpc_agg.copy()\n",
    "\n",
    "# Perform sentence embedding on the 'keyword_yake' (PatentsView) or 'keywords_yake_claims' (EPO) column\n",
    "df_keywords_list_cpc_embeddings['keyword_yake_patentsberta_embedding'] = model_patentsberta.encode(df_keywords_list_cpc_embeddings['keyword_yake_lemma'], show_progress_bar=True).tolist()\n",
    "\n",
    "df_keywords_list_cpc_embeddings['keyword_yake_climatebert_embedding'] = model_climatebert.encode(df_keywords_list_cpc_embeddings['keyword_yake_lemma'], show_progress_bar=True).tolist()\n",
    "\n",
    "df_keywords_list_cpc_embeddings['keyword_yake_bertforpatents_embedding'] = model_bertforpatents.encode(df_keywords_list_cpc_embeddings['keyword_yake_lemma'], show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_cpc_embeddings.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/cpc_yake_keywords_list_noun_chunks_embeddings.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge EP, USPTO and Reliance on Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_keywords_list_uspto_agg = pd.read_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/uspto_yake_keywords_list_noun_chunks.json')\n",
    "df_keywords_list_ep_agg = pd.read_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/epo_yake_keywords_list_noun_chunks.json')\n",
    "df_keywords_list_rel_agg = pd.read_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/rel_on_science_yake_keywords_list_noun_chunks.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_uspto_agg = pd.read_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/uspto_yake_keywords_list_non_cleantech_noun_chunks.json')\n",
    "df_keywords_list_ep_agg = pd.read_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/epo_yake_keywords_list_non_cleantech_noun_chunks.json')\n",
    "df_keywords_list_rel_agg = pd.read_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/rel_on_science_yake_keywords_list_non_cleantech_noun_chunks.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382749/382749 [00:00<00:00, 882729.64it/s]\n",
      "100%|██████████| 989489/989489 [00:02<00:00, 394997.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cast publn_nr and patent_id to list of strings\n",
    "df_keywords_list_ep_agg['publn_nr'] = df_keywords_list_ep_agg['publn_nr'].progress_apply(lambda x: [str(item) for item in x])\n",
    "df_keywords_list_uspto_agg['patent_id'] = df_keywords_list_uspto_agg['patent_id'].progress_apply(lambda x: [str(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_keywords_list_uspto_agg, df_keywords_list_ep_agg, df_keywords_list_rel_agg]\n",
    "df_keywords_list = pd.concat(frames)\n",
    "df_keywords_list.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_yake_lemma</th>\n",
       "      <th>yake_conf_score</th>\n",
       "      <th>patent_id</th>\n",
       "      <th>abs_frequency</th>\n",
       "      <th>publn_nr</th>\n",
       "      <th>oaid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2255156</th>\n",
       "      <td>point monitoring system</td>\n",
       "      <td>0.093762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2027892146]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009278</th>\n",
       "      <td>annular ptfe seal</td>\n",
       "      <td>0.008866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[2705284]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2131224</th>\n",
       "      <td>muscle myoglobin concentration</td>\n",
       "      <td>0.173772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2101851266]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452590</th>\n",
       "      <td>assert</td>\n",
       "      <td>0.189831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2931856967, 2996602250]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998908</th>\n",
       "      <td>aerosol composition</td>\n",
       "      <td>0.019367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>[1157689, 1297096]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     keyword_yake_lemma  yake_conf_score patent_id  \\\n",
       "2255156         point monitoring system         0.093762       NaN   \n",
       "1009278               annular ptfe seal         0.008866       NaN   \n",
       "2131224  muscle myoglobin concentration         0.173772       NaN   \n",
       "1452590                          assert         0.189831       NaN   \n",
       "998908              aerosol composition         0.019367       NaN   \n",
       "\n",
       "         abs_frequency            publn_nr                      oaid  \n",
       "2255156              1                 NaN              [2027892146]  \n",
       "1009278              1           [2705284]                       NaN  \n",
       "2131224              1                 NaN              [2101851266]  \n",
       "1452590              2                 NaN  [2931856967, 2996602250]  \n",
       "998908               2  [1157689, 1297096]                       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keywords_list.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_lists(series):\n",
    "    combined_list = []\n",
    "    for item in series:\n",
    "        if isinstance(item, list):\n",
    "            combined_list.extend(item)\n",
    "    return combined_list\n",
    "\n",
    "# Group by 'keyword_yake_lemma' and aggregate\n",
    "df_keywords_list_agg = df_keywords_list.groupby('keyword_yake_lemma').agg({\n",
    "    'yake_conf_score': 'mean',        # Mean of yake_conf_score\n",
    "    'abs_frequency': 'sum',           # Sum of abs_frequency\n",
    "    'patent_id': concat_lists,        # Concatenate lists in patent_id\n",
    "    'publn_nr': concat_lists,         # Concatenate lists in publn_nr\n",
    "    'oaid': concat_lists,             # Concatenate lists in oaid\n",
    "    # 'cpc_group': concat_lists,        # Concatenate lists in cpc_group\n",
    "    # 'cpc_class_symbol': concat_lists  # Concatenate lists in cpc_class_symbol\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2371133/2371133 [00:03<00:00, 643448.42it/s] \n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def flatten_and_convert(entry):\n",
    "    # If the entry is NaN (float type in pandas), return an empty list\n",
    "    if isinstance(entry, float):\n",
    "        return []\n",
    "\n",
    "    # Initialize an empty list to store the flattened results\n",
    "    flattened_list = []\n",
    "\n",
    "    # Check if the entry is a string and convert it to a list if it represents a list\n",
    "    if isinstance(entry, str) and entry.startswith(\"[\") and entry.endswith(\"]\"):\n",
    "        try:\n",
    "            entry = ast.literal_eval(entry)\n",
    "        except ValueError:\n",
    "            # If conversion fails, return an empty list\n",
    "            return []\n",
    "\n",
    "    # If the entry is a list, process its items\n",
    "    if isinstance(entry, list):\n",
    "        for item in entry:\n",
    "            # If the item is a string representation of a list, convert it\n",
    "            if isinstance(item, str) and item.startswith(\"[\") and item.endswith(\"]\"):\n",
    "                try:\n",
    "                    item = ast.literal_eval(item)\n",
    "                except ValueError:\n",
    "                    continue  # Skip items that can't be converted\n",
    "\n",
    "            # If the item is a list, extend the flattened list with its elements\n",
    "            if isinstance(item, list):\n",
    "                flattened_list.extend(item)\n",
    "            else:\n",
    "                # For single string items, append them directly\n",
    "                flattened_list.append(item)\n",
    "\n",
    "    return flattened_list\n",
    "\n",
    "# Apply the function to the 'publn_nr' column\n",
    "df_keywords_list_agg['publn_nr'] = df_keywords_list_agg['publn_nr'].progress_apply(flatten_and_convert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2371133/2371133 [00:01<00:00, 1482898.88it/s]\n",
      "100%|██████████| 2371133/2371133 [00:03<00:00, 632148.56it/s] \n",
      "100%|██████████| 2371133/2371133 [00:01<00:00, 1431137.37it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(lst):\n",
    "    # Convert list to set to remove duplicates, then back to list\n",
    "    return list(set(lst))\n",
    "\n",
    "# Apply the function to each relevant column\n",
    "df_keywords_list_agg['patent_id'] = df_keywords_list_agg['patent_id'].progress_apply(remove_duplicates)\n",
    "df_keywords_list_agg['publn_nr'] = df_keywords_list_agg['publn_nr'].progress_apply(remove_duplicates)\n",
    "df_keywords_list_agg['oaid'] = df_keywords_list_agg['oaid'].progress_apply(remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2371133/2371133 [00:03<00:00, 615107.09it/s] \n",
      "100%|██████████| 2371133/2371133 [00:03<00:00, 620641.78it/s] \n",
      "100%|██████████| 2371133/2371133 [00:03<00:00, 668795.59it/s] \n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates_from_lists(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].progress_apply(lambda x: list(set(x)))\n",
    "    return df\n",
    "\n",
    "# Apply this function to the 'oaid', 'patent_id', and 'publn_nr' columns\n",
    "# df_keywords_list_agg = remove_duplicates_from_lists(df_keywords_list_agg, ['oaid', 'patent_id', 'publn_nr', 'cpc_group', 'cpc_class_symbol'])\n",
    "df_keywords_list_agg = remove_duplicates_from_lists(df_keywords_list_agg, ['oaid', 'patent_id', 'publn_nr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2371133it [01:07, 35380.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count number of 'patent_id', 'publn_nr', and 'oaid'\n",
    "patent_id_list = []\n",
    "publn_nr_list = []\n",
    "oaid_list = []\n",
    "\n",
    "for index, row in tqdm(df_keywords_list_agg.iterrows()):\n",
    "    # Append to lists\n",
    "    patent_id_list.extend(row['patent_id'])\n",
    "    publn_nr_list.extend(row['publn_nr'])\n",
    "    oaid_list.extend(row['oaid'])\n",
    "    \n",
    "# Remove duplicates\n",
    "patent_id_list = list(set(patent_id_list))\n",
    "publn_nr_list = list(set(publn_nr_list))\n",
    "oaid_list = list(set(oaid_list))\n",
    "\n",
    "count_doc = len(patent_id_list) + len(publn_nr_list) + len(oaid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1378112"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune dataframe by document frequency and absolute frequency\n",
    "min_abs_frequency = 5\n",
    "max_abs_frequency = 1000\n",
    "min_doc_frequency = min_abs_frequency / count_doc\n",
    "max_doc_frequency = max_abs_frequency / count_doc\n",
    "\n",
    "df_keywords_list_agg_pruned = df_keywords_list_agg[(df_keywords_list_agg['abs_frequency'] >= min_abs_frequency) & (df_keywords_list_agg['abs_frequency'] <= max_abs_frequency)]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_agg_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149184"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_keywords_list_agg_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/anferico_bert-for-patents. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4662/4662 [00:32<00:00, 143.14it/s]\n",
      "Batches: 100%|██████████| 4662/4662 [00:19<00:00, 243.26it/s]\n",
      "Batches: 100%|██████████| 4662/4662 [01:26<00:00, 54.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate copy of df_claims_keywords_list\n",
    "df_keywords_list_agg_embeddings = df_keywords_list_agg_pruned.copy()\n",
    "\n",
    "# Perform sentence embedding on the 'keyword_yake' (PatentsView) or 'keywords_yake_claims' (EPO) column\n",
    "df_keywords_list_agg_embeddings['keyword_yake_patentsberta_embedding'] = model_patentsberta.encode(df_keywords_list_agg_embeddings['keyword_yake_lemma'], show_progress_bar=True).tolist()\n",
    "\n",
    "df_keywords_list_agg_embeddings['keyword_yake_climatebert_embedding'] = model_climatebert.encode(df_keywords_list_agg_embeddings['keyword_yake_lemma'], show_progress_bar=True).tolist()\n",
    "\n",
    "df_keywords_list_agg_embeddings['keyword_yake_bertforpatents_embedding'] = model_bertforpatents.encode(df_keywords_list_agg_embeddings['keyword_yake_lemma'], show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to json\n",
    "# df_keywords_list_agg_embeddings.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keywords_list_agg_uspto_epo_rel_embeddings_noun_chunks.json', orient='records')\n",
    "df_keywords_list_agg_embeddings.to_json('/mnt/hdd01/patentsview/Non Cleantech Patents - Classifier Set/uspto_epo_rel_keywords_list_non_cleantech_noun_chunks_processed_embeddings.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
