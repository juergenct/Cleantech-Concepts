{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Cleantech Keyword List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lemmatize(text):\n",
    "    \"\"\"\n",
    "    Normalize, clean, and lemmatize the input text.\n",
    "\n",
    "    :param text: A string containing the text to be processed.\n",
    "    :return: A string representing the processed text.\n",
    "    \"\"\"\n",
    "    # Normalize the text with unicodedata\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove URLs, brackets, and non-alphabetic characters; convert to lowercase\n",
    "    text = re.sub(r\"\\[.*?\\]|\\(.*?\\)|\\{.*?\\}\", \"\", text)\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z- ]\", \" \", text).lower().strip()\n",
    "\n",
    "    # Lemmatize each word\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare Cleantech Data\n",
    "# Co-Occurrence Directory\n",
    "# co_occurrence_dir = '/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Co-Occurrence Analysis/'\n",
    "# co_occurrence_files = glob.glob(co_occurrence_dir + '*.csv')\n",
    "co_occurrence_files = '/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Co-Occurrence Analysis/co_occurrence_matrix_yake_keywords_cleantech_uspto_epo_rel_ids_semantic_similarity_02.csv'\n",
    "\n",
    "# Similarity Directory\n",
    "# similarity_dir = '/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Similarity Search/'\n",
    "# similarity_files = glob.glob(similarity_dir + '*.json')\n",
    "similarity_files = '/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Similarity Search/df_keyword_titles_cosine_similarity_radius_025_neighbors_100_noun_chunks.json'\n",
    "\n",
    "# Co-Occurrence Threshold\n",
    "# co_occurrence_threshold = [0.01, 0.025, 0.05, 0.1, 0.15]\n",
    "co_occurrence_threshold = [0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22701/2755511391.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  mask = df_cleantech_cooccurrence.applymap(lambda x: x >= co_occurrence_threshold)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df_cleantech_cooccurrence = pd.read_csv(co_occurrence_files, index_col=0)\n",
    "df_cleantech_cooccurrence.dropna(how='all', inplace=True)\n",
    "\n",
    "df_cleantech_similarity = pd.read_json(similarity_files)\n",
    "\n",
    "# Co-Occurrence Threshold\n",
    "co_occurrence_threshold = 0.01  # Assuming you are using a single threshold value\n",
    "\n",
    "# Create a mask for the co-occurrence threshold\n",
    "mask = df_cleantech_cooccurrence.applymap(lambda x: x >= co_occurrence_threshold)\n",
    "\n",
    "# Apply mask to DataFrame\n",
    "filtered_co_occurrence_df = df_cleantech_cooccurrence[mask]\n",
    "\n",
    "# Extract keywords\n",
    "co_occurrence_list = filtered_co_occurrence_df.columns[filtered_co_occurrence_df.any()].tolist()\n",
    "\n",
    "# Processing similarity data\n",
    "similarity_series = pd.concat([df_cleantech_similarity['keyword_yake_lemma'], df_cleantech_similarity['keywords_keyword_yake_bertforpatents_embedding'].explode()], ignore_index=True)\n",
    "similarity_list = similarity_series.drop_duplicates().tolist()\n",
    "\n",
    "# Combine and deduplicate lists\n",
    "cleantech_list = list(set(co_occurrence_list + similarity_list))\n",
    "cleantech_list = [str(keyword) for keyword in cleantech_list]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df_cleantech = pd.DataFrame(cleantech_list, columns=['keyword_yake_lemma'])\n",
    "# df_cleantech['cleantech'] = 1\n",
    "\n",
    "del df_cleantech_cooccurrence\n",
    "del df_cleantech_similarity\n",
    "del co_occurrence_list\n",
    "del similarity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute TF-IDF Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_uspto_cleantech = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/g_patent_claims_fulltext_cleantech.json')\n",
    "g_uspto_cleantech['patent_id'] = 'us-' + g_uspto_cleantech['patent_id'].astype(str)\n",
    "g_uspto_cleantech['claim_fulltext'] = g_uspto_cleantech['claim_fulltext'].apply(clean_and_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document List for HashingVectorizer\n",
    "document_list = g_uspto_cleantech['claim_fulltext'].tolist()\n",
    "\n",
    "# Cast to string using a generator expression\n",
    "document_list = (str(x) for x in document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HashingVectorizer\n",
    "vectorizer_uspto_cleantech = HashingVectorizer(n_features=len(cleantech_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document Term Matrix\n",
    "document_term_matrix_uspto_cleantech = vectorizer_uspto_cleantech.fit_transform(document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Document Term Matrix to a Compressed Sparse Column matrix\n",
    "document_term_matrix_uspto_cleantech = csr_matrix(document_term_matrix_uspto_cleantech.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df_uspto_cleantech = pd.DataFrame.sparse.from_spmatrix(document_term_matrix_uspto_cleantech,\n",
    "                                                       columns=g_uspto_cleantech['patent_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame\n",
    "df_uspto_cleantech.to_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/df_uspto_cleantech_hashing.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
