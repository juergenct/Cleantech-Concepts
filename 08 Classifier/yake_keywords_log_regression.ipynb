{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Matrices for the Sci-Kit Learn Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Cleantech Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare Cleantech Data\n",
    "co_occurrence_file = '/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Co-Occurrence Analysis/co_occurrence_matrix_yake_keywords_cleantech_uspto_epo_rel_ids_semantic_similarity_02.csv'\n",
    "similarity_file = '/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Similarity Search/df_keyword_titles_cosine_similarity_radius_015_neighbors_100_noun_chunks.json'\n",
    "\n",
    "df_cleantech_cooccurrence = pd.read_csv(co_occurrence_file, index_col=0)\n",
    "df_cleantech_cooccurrence.dropna(how='all', inplace=True)\n",
    "\n",
    "df_cleantech_similarity = pd.read_json(similarity_file)\n",
    "\n",
    "# Co-Occurrence Threshold\n",
    "co_occurrence_threshold = 0.1\n",
    "\n",
    "# Create a mask for the co-occurrence threshold\n",
    "mask = df_cleantech_cooccurrence >= co_occurrence_threshold\n",
    "\n",
    "# Apply mask to DataFrame\n",
    "filtered_co_occurrence_df = df_cleantech_cooccurrence[mask]\n",
    "\n",
    "# Extract keywords\n",
    "co_occurrence_list = filtered_co_occurrence_df.columns[filtered_co_occurrence_df.any()].tolist()\n",
    "\n",
    "# Processing similarity data\n",
    "similarity_series = pd.concat([df_cleantech_similarity['keyword_yake_lemma'], df_cleantech_similarity['keywords_keyword_yake_bertforpatents_embedding'].explode()], ignore_index=True)\n",
    "\n",
    "# Drop duplicates before converting to list\n",
    "similarity_list = similarity_series.drop_duplicates().tolist()\n",
    "\n",
    "# Combine and deduplicate lists\n",
    "cleantech_list = list(set(co_occurrence_list + similarity_list))\n",
    "cleantech_list = [str(keyword) for keyword in cleantech_list]\n",
    "\n",
    "# Create DataFrame\n",
    "df_cleantech = pd.DataFrame(cleantech_list, columns=['keyword_yake_lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179387/179387 [00:12<00:00, 14111.48it/s]\n",
      "100%|██████████| 515602/515602 [01:06<00:00, 7791.12it/s] \n",
      "100%|██████████| 618506/618506 [00:17<00:00, 35136.77it/s]\n"
     ]
    }
   ],
   "source": [
    "g_epo_cleantech = pd.read_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/g_epo_cleantech_trie.csv')\n",
    "g_uspto_cleantech = pd.read_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/g_uspto_cleantech_trie.csv')\n",
    "g_rel_cleantech = pd.read_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/g_rel_cleantech_trie.csv')\n",
    "\n",
    "# Delete all rows where trie is NaN or empty\n",
    "g_epo_cleantech.dropna(subset=['trie'], inplace=True)\n",
    "g_uspto_cleantech.dropna(subset=['trie'], inplace=True)\n",
    "g_rel_cleantech.dropna(subset=['trie'], inplace=True)\n",
    "\n",
    "# Concatenate list of strings in trie column to a single string\n",
    "g_epo_cleantech['trie'] = g_epo_cleantech['trie'].progress_apply(lambda x: ' '.join(eval(x)))\n",
    "g_uspto_cleantech['trie'] = g_uspto_cleantech['trie'].progress_apply(lambda x: ' '.join(eval(x)))\n",
    "g_rel_cleantech['trie'] = g_rel_cleantech['trie'].progress_apply(lambda x: ' '.join(eval(x)))\n",
    "\n",
    "# Concatenate the three DataFrames\n",
    "g_cleantech = pd.concat([g_epo_cleantech, g_uspto_cleantech, g_rel_cleantech], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Non Cleantech Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179012/179012 [00:12<00:00, 14538.11it/s]\n",
      "100%|██████████| 599676/599676 [01:14<00:00, 8051.56it/s] \n",
      "100%|██████████| 606015/606015 [00:16<00:00, 35975.17it/s]\n"
     ]
    }
   ],
   "source": [
    "g_epo_non_cleantech = pd.read_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/g_epo_non_cleantech_trie.csv')\n",
    "g_uspto_non_cleantech = pd.read_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/g_uspto_non_cleantech_trie.csv')\n",
    "g_rel_non_cleantech = pd.read_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/TFIDF Matrices/g_rel_non_cleantech_trie.csv')\n",
    "\n",
    "# Delete all rows where trie is NaN or empty\n",
    "g_epo_non_cleantech.dropna(subset=['trie'], inplace=True)\n",
    "g_uspto_non_cleantech.dropna(subset=['trie'], inplace=True)\n",
    "g_rel_non_cleantech.dropna(subset=['trie'], inplace=True)\n",
    "\n",
    "# Concatenate list of strings in trie column to a single string\n",
    "g_epo_non_cleantech['trie'] = g_epo_non_cleantech['trie'].progress_apply(lambda x: ' '.join(eval(x)))\n",
    "g_uspto_non_cleantech['trie'] = g_uspto_non_cleantech['trie'].progress_apply(lambda x: ' '.join(eval(x)))\n",
    "g_rel_non_cleantech['trie'] = g_rel_non_cleantech['trie'].progress_apply(lambda x: ' '.join(eval(x)))\n",
    "\n",
    "# Concatenate the three DataFrames\n",
    "g_non_cleantech = pd.concat([g_epo_non_cleantech, g_uspto_non_cleantech, g_rel_non_cleantech], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CountVectorizer Matrix for the Sci-Kit Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = CountVectorizer(\n",
    "    vocabulary = cleantech_list,\n",
    "    ngram_range = (1, 4),\n",
    "    # max_df = 0.5,\n",
    "    # min_df = 0.01,\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    ")\n",
    "# scaler = StandardScaler(with_mean=False)  # with_mean=False to support sparse matrices\n",
    "\n",
    "g_cleantech_matrix = Vectorizer.fit_transform(g_cleantech['trie'])\n",
    "# g_cleantech_matrix = scaler.fit_transform(g_cleantech_matrix)\n",
    "\n",
    "g_non_cleantech_matrix = Vectorizer.fit_transform(g_non_cleantech['trie'])\n",
    "# g_non_cleantech_matrix = scaler.fit_transform(g_non_cleantech_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a given model\n",
    "def train_evaluate_model(model, X_train, X_test, y_train, y_test, df_cleantech):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "    feature_importance = model.coef_[0]\n",
    "    feature_names = Vectorizer.get_feature_names_out()\n",
    "    keywords_importance = zip(feature_names, feature_importance)\n",
    "    sorted_keywords = sorted(keywords_importance, key=lambda x: x[1], reverse=True)\n",
    "    df_keywords_importance = pd.DataFrame(sorted_keywords, columns=['keyword_yake_lemma', 'logistic_regression_importance'])\n",
    "    df_cleantech = pd.merge(df_cleantech, df_keywords_importance, on='keyword_yake_lemma', how='left')\n",
    "    \n",
    "    return df_cleantech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data for train_test split\n",
    "X = vstack([g_cleantech_matrix, g_non_cleantech_matrix])\n",
    "y = np.concatenate((np.ones(g_cleantech_matrix.shape[0]), np.zeros(g_non_cleantech_matrix.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.91      0.73    276531\n",
      "         1.0       0.80      0.39      0.52    263109\n",
      "\n",
      "    accuracy                           0.66    539640\n",
      "   macro avg       0.71      0.65      0.63    539640\n",
      "weighted avg       0.70      0.66      0.63    539640\n",
      "\n",
      "Confusion Matrix:\n",
      " [[251235  25296]\n",
      " [160873 102236]]\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "df_cleantech = train_evaluate_model(model, X_train, X_test, y_train, y_test, df_cleantech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              keyword_yake_lemma  logistic_regression_importance\n",
      "1241              fuel cell fuel                       -1.326671\n",
      "2233           fuel cell segment                       -1.124320\n",
      "531            microbial biomass                       -0.873696\n",
      "2333            slope efficiency                       -0.810049\n",
      "8                       dyestuff                       -0.747549\n",
      "1423    abiotic stress tolerance                       -0.726329\n",
      "2503   sewage treatment facility                       -0.696200\n",
      "1607                   adversary                       -0.668568\n",
      "295   reactive power compensator                       -0.617369\n",
      "997                algal biomass                       -0.580287\n",
      "1439  single crystalline silicon                       -0.555670\n",
      "1173                        hals                       -0.540860\n",
      "930               abiotic stress                       -0.499333\n",
      "1728                caprolactone                       -0.495766\n",
      "13        turbine electric power                       -0.489279\n",
      "495         efficient combustion                       -0.463722\n",
      "715            wind energy plant                       -0.447047\n",
      "2065                        cyan                       -0.438098\n",
      "1140     energy storage facility                       -0.423124\n",
      "633     ionic liquid composition                       -0.408268\n",
      "2550           smart grid device                       -0.406336\n",
      "47           biomass composition                       -0.389130\n",
      "498     energy efficient heating                       -0.386846\n",
      "1062            presented method                       -0.384313\n",
      "204          treated waste water                       -0.384160\n"
     ]
    }
   ],
   "source": [
    "# Print first 25 entries sorted by importance\n",
    "print(df_cleantech.sort_values(by='logistic_regression_importance', ascending=True).head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2796"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_cleantech)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
