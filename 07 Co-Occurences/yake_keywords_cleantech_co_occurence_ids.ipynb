{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yake_uspto_epo_rel = pd.read_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keywords_list_agg_uspto_epo_rel_embeddings_noun_chunks.json')\n",
    "# Drop columns keyword_yake_pos, keyword_yake_patentsberta_embedding, keyword_yake_climatebert_embedding and keyword_yake_bertforpatents_embedding\n",
    "df_yake_uspto_epo_rel.drop(columns=['keyword_yake_patentsberta_embedding', 'keyword_yake_climatebert_embedding', 'keyword_yake_bertforpatents_embedding'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yake_cleantech_titles = pd.read_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/cpc_yake_keywords_list_noun_chunks_embeddings.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique keywords\n",
    "cleantech_titles = df_yake_cleantech_titles['keyword_yake_lemma'].unique()\n",
    "uspto_epo_rel_keywords = df_yake_uspto_epo_rel['keyword_yake_lemma'].unique()\n",
    "\n",
    "# Initialize co-occurrence matrix\n",
    "co_occurrence_matrix = pd.DataFrame(0, index=cleantech_titles, columns=uspto_epo_rel_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the 'patent_id' column while retaining the 'keyword_yake' column\n",
    "patent_id_exploded = df_yake_uspto_epo_rel.set_index('keyword_yake_lemma')['patent_id'].explode().reset_index()\n",
    "# Put 'US' in front of each patent_id\n",
    "patent_id_exploded['patent_id'] = 'US' + patent_id_exploded['patent_id'].astype(str)\n",
    "# Delete all rows where patent_id contains the string None\n",
    "patent_id_exploded = patent_id_exploded[~patent_id_exploded['patent_id'].str.contains('None')]\n",
    "# Aggregate on 'patent_id', list of keyword_yake_lemma\n",
    "patent_id_exploded = patent_id_exploded.groupby('patent_id')['keyword_yake_lemma'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_id_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the 'publn_nr' column while retaining the 'keyword_yake' column\n",
    "publn_nr_exploded = df_yake_uspto_epo_rel.set_index('keyword_yake_lemma')['publn_nr'].explode().reset_index()\n",
    "# Put 'EP' in front of each publn_nr\n",
    "publn_nr_exploded['publn_nr'] = 'EP' + publn_nr_exploded['publn_nr'].astype(str)\n",
    "# Delete all rows where publn_nr contains the string None\n",
    "publn_nr_exploded = publn_nr_exploded[~publn_nr_exploded['publn_nr'].str.contains('None')]\n",
    "# Aggregate on 'publn_nr', list all keyword_yake_lemma\n",
    "publn_nr_exploded = publn_nr_exploded.groupby('publn_nr')['keyword_yake_lemma'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publn_nr_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the 'oaid' column while retaining the 'keyword_yake' column\n",
    "oaid_exploded = df_yake_uspto_epo_rel.set_index('keyword_yake_lemma')['oaid'].explode().reset_index()\n",
    "# Put 'REL' in front of each oaid\n",
    "oaid_exploded['oaid'] = 'REL' + oaid_exploded['oaid'].astype(str)\n",
    "# Delete all rows where oaid contains the string None\n",
    "oaid_exploded = oaid_exploded[~oaid_exploded['oaid'].str.contains('None')]\n",
    "# Aggregate on 'oaid', list of 'keyword_yake_lemma'\n",
    "oaid_exploded = oaid_exploded.groupby('oaid')['keyword_yake_lemma'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oaid_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the three exploded dataframes\n",
    "df_keywords_uspto_epo_rel = pd.concat([patent_id_exploded, publn_nr_exploded, oaid_exploded], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_uspto_epo_rel.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the 'keyword_yake_lemma' column\n",
    "df_keywords_uspto_epo_rel_exploded = df_keywords_uspto_epo_rel.explode('keyword_yake_lemma').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_keywords_uspto_epo_rel_exploded with df_yake_cleantech_titles on 'keyword_yake_lemma'and keep patent_id, publn_nr and oaid, including duplicates\n",
    "df_yake_uspto_epo_rel_titles_filtered = pd.merge(df_yake_cleantech_titles, df_keywords_uspto_epo_rel_exploded, how='inner', left_on='keyword_yake_lemma', right_on='keyword_yake_lemma', validate='many_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows corresponding to 'keyword_yake_lemma' when keyword_yake_lemma occurs less than x times\n",
    "co_occurrence_threshold = 100\n",
    "co_occurrence = df_yake_uspto_epo_rel_titles_filtered['keyword_yake_lemma'].value_counts()\n",
    "co_occurrence = co_occurrence[co_occurrence > co_occurrence_threshold]\n",
    "co_occurrence = co_occurrence.reset_index()\n",
    "co_occurrence.columns = ['keyword_yake_lemma', 'count']\n",
    "df_yake_uspto_epo_rel_titles_filtered = df_yake_uspto_epo_rel_titles_filtered.merge(co_occurrence, how='inner', left_on='keyword_yake_lemma', right_on='keyword_yake_lemma', validate='many_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yake_uspto_epo_rel_titles_filtered['keyword_yake_lemma'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Occurence Matrix Single Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess df_keywords_uspto_epo_rel to create dictionaries for fast lookup\n",
    "patent_id_dict = df_keywords_uspto_epo_rel.groupby('patent_id')['keyword_yake_lemma'].progress_apply(list).to_dict()\n",
    "publn_nr_dict = df_keywords_uspto_epo_rel.groupby('publn_nr')['keyword_yake_lemma'].progress_apply(list).to_dict()\n",
    "oaid_dict = df_keywords_uspto_epo_rel.groupby('oaid')['keyword_yake_lemma'].progress_apply(list).to_dict()\n",
    "\n",
    "# Function to update co-occurrence matrix\n",
    "def update_co_occurrence(row):\n",
    "    if isinstance(row['patent_id'], str) and row['patent_id'] in patent_id_dict:\n",
    "        keyword_lists = patent_id_dict[row['patent_id']]\n",
    "    elif isinstance(row['publn_nr'], str) and row['publn_nr'] in publn_nr_dict:\n",
    "        keyword_lists = publn_nr_dict[row['publn_nr']]\n",
    "    elif isinstance(row['oaid'], str) and row['oaid'] in oaid_dict:\n",
    "        keyword_lists = oaid_dict[row['oaid']]\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    for keyword_list in keyword_lists:\n",
    "        for keyword in keyword_list:\n",
    "            co_occurrence_matrix.at[row['keyword_yake_lemma'], keyword] += 1\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "for index, row in tqdm(df_yake_uspto_epo_rel_titles_filtered.iterrows(), total=len(df_yake_uspto_epo_rel_titles_filtered)):\n",
    "    update_co_occurrence(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print descending columns for row 'xxx'\n",
    "co_occurrence_matrix.loc['wastewater treatment'].sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide rows in co-occurence matrix by largest value in row\n",
    "co_occurrence_matrix = co_occurrence_matrix.div(co_occurrence_matrix.max(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrence_matrix.to_csv('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/Co-Occurence Analysis/co_occurrence_matrix_yake_keywords_cleantech_uspto_epo_rel.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Occurence Matrix Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, df_keywords_uspto_epo_rel, co_occurrence_matrix):\n",
    "    for index, row in chunk.iterrows():\n",
    "        if isinstance(row['patent_id'], str):\n",
    "            for keyword_list in df_keywords_uspto_epo_rel[df_keywords_uspto_epo_rel['patent_id'] == row['patent_id']]['keyword_yake_lemma']:\n",
    "                for keyword in keyword_list:\n",
    "                    co_occurrence_matrix.loc[row['keyword_yake_lemma'], keyword] += 1\n",
    "        elif isinstance(row['publn_nr'], str):\n",
    "            for keyword_list in df_keywords_uspto_epo_rel[df_keywords_uspto_epo_rel['publn_nr'] == row['publn_nr']]['keyword_yake_lemma']:\n",
    "                for keyword in keyword_list:\n",
    "                    co_occurrence_matrix.loc[row['keyword_yake_lemma'], keyword] += 1\n",
    "        elif isinstance(row['oaid'], str):\n",
    "            for keyword_list in df_keywords_uspto_epo_rel[df_keywords_uspto_epo_rel['oaid'] == row['oaid']]['keyword_yake_lemma']:\n",
    "                for keyword in keyword_list:\n",
    "                    co_occurrence_matrix.loc[row['keyword_yake_lemma'], keyword] += 1\n",
    "        else:\n",
    "            print('Error')\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "def main(df_yake_uspto_epo_rel, df_keywords_uspto_epo_rel, co_occurrence_matrix):\n",
    "    num_cores = 6\n",
    "    chunk_size = len(df_yake_uspto_epo_rel) // num_cores\n",
    "\n",
    "    # Create a list of DataFrame chunks\n",
    "    chunks = [df_yake_uspto_epo_rel.iloc[i:i + chunk_size] for i in range(0, df_yake_uspto_epo_rel.shape[0], chunk_size)]\n",
    "\n",
    "    # Set up a multiprocessing Pool\n",
    "    with Pool(num_cores) as pool:\n",
    "        results = list(tqdm(pool.starmap(process_chunk, [(chunk, df_keywords_uspto_epo_rel, co_occurrence_matrix.copy()) for chunk in chunks]), total=len(chunks)))\n",
    "\n",
    "    # Combine the results\n",
    "    for matrix in results:\n",
    "        co_occurrence_matrix += matrix\n",
    "\n",
    "    return co_occurrence_matrix\n",
    "\n",
    "# Call the main function with appropriate arguments\n",
    "co_occurrence_matrix = main(df_yake_uspto_epo_rel, df_keywords_uspto_epo_rel, co_occurrence_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
