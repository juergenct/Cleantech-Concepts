{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORK ON THIS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_yake_claims = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [01:11, 7207.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Cast row 'keywords_yake' column to string\n",
    "    row['keywords_yake'] = str(row['keywords_yake'])\n",
    "    # Check if keywords_yake column starts with \"[[[\" and ends with \"]]]\"\"\n",
    "    if row['keywords_yake'].startswith('[[[') and row['keywords_yake'].endswith(']]]'):\n",
    "        # Remove first \"[\" and last \"]\" from keywords_yake column\n",
    "        row['keywords_yake'] = row['keywords_yake'][1:-1]\n",
    "    # Cast row 'keywords_yake' column to list\n",
    "    row['keywords_yake'] = ast.literal_eval(row['keywords_yake'])\n",
    "    # Assign modified 'keywords_yake' list to temporary variable\n",
    "    keywords_yake_temp = row['keywords_yake']\n",
    "    # Assign temporary variable to 'keywords_yake' column\n",
    "    df_yake_claims.at[index, 'keywords_yake'] = keywords_yake_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [02:04, 4156.10it/s]\n"
     ]
    }
   ],
   "source": [
    "df_yake_claims['keywords_yake_exploded'] = ''\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['keywords_yake'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append to 'keywords_yake_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake']:\n",
    "            df_yake_claims.at[index, 'keywords_yake_exploded'] += keyword[0] + ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'keywords_yake_exploded' column to list\n",
    "df_yake_claims['keywords_yake_exploded'] = df_yake_claims['keywords_yake_exploded'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete last item in 'keywords_yake_exploded' cells\n",
    "df_yake_claims['keywords_yake_exploded'] = df_yake_claims['keywords_yake_exploded'].apply(lambda x: x[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [00:12, 41248.83it/s]\n"
     ]
    }
   ],
   "source": [
    "keywords_list = []\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Append 'keywords_yake_exploded' to keywords_list\n",
    "    keywords_list += row['keywords_yake_exploded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique list of keywords\n",
    "keywords_list_unique = list(set(keywords_list))\n",
    "df_keywords_list_unique = pd.DataFrame(keywords_list_unique, columns=['keyword_yake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/515742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515742/515742 [00:00<00:00, 2566097.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count absolute frequency of each keyword\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Flatten the list of keywords into a single list\n",
    "flattened_keywords = [keyword for keyword_list in tqdm(df_yake_claims['keywords_yake_exploded']) for keyword in keyword_list]\n",
    "\n",
    "# Step 2: Use collections.Counter to count the occurrences of each keyword\n",
    "keyword_counts = Counter(flattened_keywords)\n",
    "\n",
    "# Step 3: Convert the keyword counts to a DataFrame\n",
    "count_df = pd.DataFrame(keyword_counts.items(), columns=['keyword_yake', 'abs_frequency'])\n",
    "\n",
    "# Step 4: Merge the count_df with df_keywords_list_unique on the 'keyword_yake' column\n",
    "df_keywords_list_unique = df_keywords_list_unique.merge(count_df, on='keyword_yake', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAYBE I NEED TO REDO THIS WITH FULL TEXT CLAIMS INSTEAD OF JUST THE YAKE KEYWORDS\n",
    "\n",
    "# Calculate document frequency by dividing absolute frequency by len of df_yake_claims\n",
    "df_keywords_list_unique['doc_frequency'] = df_keywords_list_unique['abs_frequency'] / len(df_yake_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune dataframe by document frequency and absolute frequency\n",
    "min_abs_frequency = 10\n",
    "max_doc_frequency = 0.3\n",
    "\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique[(df_keywords_list_unique['abs_frequency'] >= min_abs_frequency) & (df_keywords_list_unique['doc_frequency'] <= max_doc_frequency)]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32749/32749 [00:03<00:00, 9834.87it/s] \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Generate copy of df_keywords_list_unique_pruned\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Delete all stopwords from 'keyword_yake' column\n",
    "df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in (stopwords.words('english'))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32749 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32749/32749 [00:00<00:00, 894580.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Only consider n-grams with n >= 2\n",
    "min_ngram_length = 2\n",
    "\n",
    "# Generate copy of df_keywords_list_unique_pruned\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "def filter_keywords(keyword_string):\n",
    "    # Split the keyword string into individual keywords\n",
    "    keywords = keyword_string.split(', ')\n",
    "    # Filter out keywords with fewer than 2 words\n",
    "    filtered_keywords = [keyword for keyword in keywords if len(keyword.split()) >= min_ngram_length]\n",
    "    # Join the filtered keywords back into a string\n",
    "    return ', '.join(filtered_keywords)\n",
    "\n",
    "# Create a copy of df_keywords_list_unique_pruned to avoid SettingWithCopyWarning\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Now use .loc to modify the 'keyword_yake' column\n",
    "df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(filter_keywords)\n",
    "\n",
    "# Delete rows with empty 'keyword_yake' column\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24789/24789 [00:47<00:00, 524.78it/s]\n"
     ]
    }
   ],
   "source": [
    "### MAYBE I NEED TO REDO THIS PART WITH THE CLAIM TEXTS INSTEAD OF THE KEYWORDS\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Download spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Perform part-of-speech tagging on the 'keyword_yake' column \n",
    "# and save the POS tags in a new column 'keyword_yake_pos'\n",
    "df_keywords_list_unique_pruned_pos['keyword_yake_pos'] = df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(\n",
    "    lambda x: [token.pos_ for token in nlp(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24789/24789 [00:00<00:00, 973316.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filter out all keywords that do not contain NOUN, PRON or PROPN in their POS tags\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned_pos[df_keywords_list_unique_pruned_pos['keyword_yake_pos'].progress_apply(\n",
    "    lambda x: any(pos in ['NOUN', 'PRON', 'PROPN'] for pos in x)\n",
    ")]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned_pos.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/anferico_bert-for-patents. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "# model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "# model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23742/23742 [05:08<00:00, 76.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate copy of df_keywords_list_unique_pruned_pos\n",
    "df_keywords_list_unique_pruned_embeddings = df_keywords_list_unique_pruned_pos.copy()\n",
    "\n",
    "# Perform sentence embedding on the 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned_embeddings['keyword_yake_climatebert_embedding'] = df_keywords_list_unique_pruned_embeddings['keyword_yake'].progress_apply(\n",
    "#     lambda x: model_climatebert.encode(x)\n",
    "# )\n",
    "\n",
    "df_keywords_list_unique_pruned_embeddings['keyword_yake_bertforpatents_embedding'] = df_keywords_list_unique_pruned_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_bertforpatents.encode(x)\n",
    ")\n",
    "\n",
    "# df_keywords_list_unique_pruned_embeddings['keyword_yake_patentsberta_embedding'] = df_keywords_list_unique_pruned_embeddings['keyword_yake'].progress_apply(\n",
    "#     lambda x: model_patentsberta.encode(x)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "reducer = umap.UMAP(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "# Stack all the embeddings into a 2D array\n",
    "# embeddings_climatebert = np.vstack(df_keywords_list_unique_pruned_embeddings['keyword_yake_climatebert_embedding'].values)\n",
    "embeddings_bertforpatents = np.vstack(df_keywords_list_unique_pruned_embeddings['keyword_yake_bertforpatents_embedding'].values)\n",
    "# embeddings_patentsberta = np.vstack(df_keywords_list_unique_pruned_embeddings['keyword_yake_patentsberta_embedding'].values)\n",
    "\n",
    "# Perform UMAP dimensionality reduction\n",
    "# umap_embeddings_climatebert = reducer.fit_transform(embeddings_climatebert)\n",
    "umap_embeddings_bertforpatents = reducer.fit_transform(embeddings_bertforpatents)\n",
    "# umap_embeddings_patentsberta = reducer.fit_transform(embeddings_patentsberta)\n",
    "\n",
    "# Assign the reduced dimension embeddings back to new DataFrame columns\n",
    "# df_keywords_list_unique_pruned_embeddings['keyword_yake_climatebert_embedding_umap'] = list(umap_embeddings_climatebert)\n",
    "df_keywords_list_unique_pruned_embeddings['keyword_yake_bertforpatents_embedding_umap'] = list(umap_embeddings_bertforpatents)\n",
    "# df_keywords_list_unique_pruned_embeddings['keyword_yake_patentsberta_embedding_umap'] = list(umap_embeddings_patentsberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the UMAP coordinates into separate columns\n",
    "# df_keywords_list_unique_pruned_embeddings[['climatebert_x', 'climatebert_y']] = pd.DataFrame(df_keywords_list_unique_pruned_embeddings['keyword_yake_climatebert_embedding_umap'].tolist(), index=df_keywords_list_unique_pruned_embeddings.index)\n",
    "df_keywords_list_unique_pruned_embeddings[['bertforpatents_x', 'bertforpatents_y']] = pd.DataFrame(df_keywords_list_unique_pruned_embeddings['keyword_yake_bertforpatents_embedding_umap'].tolist(), index=df_keywords_list_unique_pruned_embeddings.index)\n",
    "# df_keywords_list_unique_pruned_embeddings[['patentsberta_x', 'patentsberta_y']] = pd.DataFrame(df_keywords_list_unique_pruned_embeddings['keyword_yake_patentsberta_embedding_umap'].tolist(), index=df_keywords_list_unique_pruned_embeddings.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HDBSCAN\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_pruned_embeddings_hdbscan = df_keywords_list_unique_pruned_embeddings.copy()\n",
    "\n",
    "# Perform HDBSCAN clustering on the UMAP coordinates\n",
    "# clusterer_climatebert = hdbscan.HDBSCAN(min_cluster_size=250, min_samples=1).fit(df_keywords_list_unique_pruned_embeddings_hdbscan[['climatebert_x', 'climatebert_y']])\n",
    "clusterer_bertforpatents = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=1).fit(df_keywords_list_unique_pruned_embeddings_hdbscan[['bertforpatents_x', 'bertforpatents_y']])\n",
    "# clusterer_patentsberta = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=1).fit(df_keywords_list_unique_pruned_embeddings_hdbscan[['patentsberta_x', 'patentsberta_y']])\n",
    "\n",
    "# Assign the cluster labels back to the DataFrame\n",
    "# df_keywords_list_unique_pruned_embeddings_hdbscan['climatebert_cluster'] = clusterer_climatebert.labels_\n",
    "df_keywords_list_unique_pruned_embeddings_hdbscan['bertforpatents_cluster'] = clusterer_bertforpatents.labels_\n",
    "# df_keywords_list_unique_pruned_embeddings_hdbscan['patentsberta_cluster'] = clusterer_patentsberta.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_pruned_embeddings_hdbscan.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_postprocessed.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data points belonging to cluster -1\n",
    "# df_filtered_climatebert = df_keywords_list_unique_pruned_embeddings_hdbscan[df_keywords_list_unique_pruned_embeddings_hdbscan['climatebert_cluster'] != -1]\n",
    "df_filtered_bertforpatents = df_keywords_list_unique_pruned_embeddings_hdbscan[df_keywords_list_unique_pruned_embeddings_hdbscan['bertforpatents_cluster'] != -1]\n",
    "# df_filtered_patentsberta = df_keywords_list_unique_pruned_embeddings_hdbscan[df_keywords_list_unique_pruned_embeddings_hdbscan['patentsberta_cluster'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP embeddings in three figures, color coded by cluster\n",
    "fig_bertforpatents = px.scatter(\n",
    "    df_filtered_bertforpatents, \n",
    "    x='bertforpatents_x', \n",
    "    y='bertforpatents_y', \n",
    "    color='bertforpatents_cluster', \n",
    "    hover_data=['keyword_yake'], \n",
    "    title='UMAP projection of the YAKE keywords (bertforpatents)',\n",
    "    height=400,  # Adjust as needed\n",
    "    width=600    # Adjust as needed\n",
    ")\n",
    "fig_bertforpatents_unfiltered = px.scatter(\n",
    "    df_keywords_list_unique_pruned_embeddings_hdbscan, \n",
    "    x='bertforpatents_x', \n",
    "    y='bertforpatents_y', \n",
    "    color='bertforpatents_cluster', \n",
    "    hover_data=['keyword_yake'], \n",
    "    title='UMAP projection of the YAKE keywords (bertforpatents)',\n",
    "    height=400,  # Adjust as needed\n",
    "    width=600    # Adjust as needed\n",
    ")\n",
    "# Display the figures\n",
    "fig_bertforpatents.show()\n",
    "fig_bertforpatents_unfiltered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe of keywords and their corresponding clusters, where each row is a cluster with list of keywords\n",
    "df_keywords_clusters_bertforpatents = df_filtered_bertforpatents.groupby('bertforpatents_cluster')['keyword_yake'].apply(list).reset_index(name='keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text-generation pipeline with Flan-T5-large\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-large', device=device)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:19<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_cluster_name(keywords):\n",
    "    # Ensure the keywords are in a list format\n",
    "    keywords = keywords.split(', ') if isinstance(keywords, str) else keywords\n",
    "    # Select only the first 1500 keywords from the list\n",
    "    selected_keywords = keywords[:1500]\n",
    "    # Join the selected keywords into a string format\n",
    "    keywords_str = ', '.join(selected_keywords)\n",
    "    # Create a prompt from the selected keywords\n",
    "    prompt = f\"Come up with a specific, precise topic name, with at maximum 3 words, for the following keywords: {keywords_str}\"\n",
    "    # Generate a response using the GPT-3 model\n",
    "    response = generator(prompt, max_length=10, do_sample=True, temperature=0.8)[0]['generated_text']\n",
    "    # Extract the cluster name from the response\n",
    "    cluster_name = response\n",
    "    return cluster_name\n",
    "\n",
    "# Apply the function to the 'keywords' column to generate cluster names\n",
    "df_keywords_clusters_bertforpatents['cluster_name'] = df_keywords_clusters_bertforpatents['keywords'].progress_apply(generate_cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_clusters_bertforpatents = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_postprocessed_bertforpatents_clusters.json', orient='records')\n",
    "df_keywords_clusters_patentsberta = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_postprocessed_patentsberta_clusters.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
