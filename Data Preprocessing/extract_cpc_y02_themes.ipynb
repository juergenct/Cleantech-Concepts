{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPC Classification Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpc_y02 = pd.read_csv('/mnt/hdd01/patentsview/CPC Classification/CPCTitleList202308/cpc-section-Y_20230801.txt', sep='\\t', header=None, names=['cpc_classification','sequence', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15523it [00:00, 34729.51it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(df_cpc_y02.iterrows()):\n",
    "    # Check length of cpc_classification in order to assign sequences\n",
    "    if len(row['cpc_classification']) == 1:\n",
    "        df_cpc_y02.loc[index, 'sequence'] = -3\n",
    "    elif len(row['cpc_classification']) == 3:\n",
    "        df_cpc_y02.loc[index, 'sequence'] = -2\n",
    "    elif len(row['cpc_classification']) == 4:\n",
    "        df_cpc_y02.loc[index, 'sequence'] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where cpc_classification does not contain \"Y02\"\n",
    "df_cpc_y02 = df_cpc_y02[df_cpc_y02['cpc_classification'].str.contains('Y02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpc_y02['full_title'] = df_cpc_y02['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpc_y02.to_csv('/home/thiesen/Documents/Cleantech_Concepts/df_cpc_y02.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted the full titles without code, as hard to handle all cases in the code.\n",
    "Whenever the title starts with lower case, the full title is added by looking at the superior group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPC Classification Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpc_y02 = pd.read_json('/mnt/hdd01/patentsview/CPC Classification/df_cpc_y02_cleantech.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cpc_y02['title_lower'] = df_cpc_y02['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:00<00:00, 347.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "\n",
    "# Create a list of all the titles\n",
    "titles_lower = df_cpc_y02['title_lower'].tolist()\n",
    "\n",
    "# Specify custom parameters\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.25\n",
    "deduplication_algo = \"seqm\"\n",
    "windowSize = 5\n",
    "numOfKeywords = 10\n",
    "\n",
    "# Initialize YAKE model\n",
    "kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold,\n",
    "                                    dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords,\n",
    "                                    features=None)\n",
    "\n",
    "# Extract keywords from titles\n",
    "yake_keywords_titles_lower = []\n",
    "for title in tqdm(titles_lower):\n",
    "    keywords = kw_extractor.extract_keywords(title)\n",
    "    temp_keyword_list = []\n",
    "    for kw in keywords:\n",
    "        temp_keyword_list.append(kw)\n",
    "    yake_keywords_titles_lower.append(temp_keyword_list)\n",
    "\n",
    "# Append the YAKE keywords to the dataframe\n",
    "df_cpc_y02['yake_keywords_titles_lower'] = yake_keywords_titles_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [00:00, 9615.62it/s]\n"
     ]
    }
   ],
   "source": [
    "min_yake_conf = 0.05\n",
    "df_cpc_y02['yake_keywords_titles_filtered'] = [[] for _ in range(len(df_cpc_y02))]\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_cpc_y02.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['yake_keywords_titles_lower'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['yake_keywords_titles_lower']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column\n",
    "    else:\n",
    "        for keyword in row['yake_keywords_titles_lower']:\n",
    "            # Check if keyword[1] is greater than or equal to min_yake_conf\n",
    "            if keyword[1] <= min_yake_conf:\n",
    "                # Append keyword to 'keywords_yake_filtered' column\n",
    "                df_cpc_y02.loc[index, 'yake_keywords_titles_filtered'].append(keyword[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:00<00:00, 18334.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Generate copy of df_keywords_list_unique_pruned\n",
    "df_cpc_y02 = df_cpc_y02.copy()\n",
    "\n",
    "# Delete all stopwords from 'keyword_yake' column\n",
    "df_cpc_y02.loc[:, 'yake_keywords_titles_filtered'] = df_cpc_y02['yake_keywords_titles_filtered'].progress_apply(\n",
    "    lambda x: '' if x in stopwords.words('english') else x\n",
    ")\n",
    "\n",
    "# Remove empty rows\n",
    "df_cpc_y02 = df_cpc_y02[df_cpc_y02['yake_keywords_titles_filtered'] != '']\n",
    "\n",
    "# Reset index\n",
    "df_cpc_y02.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 340/340 [00:01<00:00, 301.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# POS tagging of 'full_title'\n",
    "df_cpc_y02['pos_tags_full_title'] = df_cpc_y02['full_title'].progress_apply(lambda x: [(token.text, token.pos_) for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:00<00:00, 54297.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_noun_phrases(keywords, pos_tags):\n",
    "    valid_keywords = []\n",
    "    for keyword in keywords:\n",
    "        # Split the keyword into words\n",
    "        keyword_words = keyword.split()\n",
    "        # Check if at least one word in the keyword string is a noun or proper noun\n",
    "        if any(any(word == tag[0] and tag[1] in ['NOUN', 'PRON', 'PROPN'] for tag in pos_tags) for word in keyword_words):\n",
    "            valid_keywords.append(keyword)\n",
    "    return valid_keywords\n",
    "\n",
    "# Apply the function to filter the keywords\n",
    "df_cpc_y02['yake_keywords_titles_pos'] = df_cpc_y02.progress_apply(lambda row: filter_noun_phrases(row['yake_keywords_titles_filtered'], row['pos_tags_full_title']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of all classifications with exactly 4 characters\n",
    "cpc_subclass = df_cpc_y02[df_cpc_y02['cpc_classification'].str.len() == 4]['cpc_classification'].tolist()\n",
    "\n",
    "df_cpc_y02_subclass = pd.DataFrame(columns=['cpc_subclass', 'yake_keywords'])\n",
    "# Set cpc_subclass to 'cpc_subclass' column\n",
    "df_cpc_y02_subclass['cpc_subclass'] = cpc_subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:00, 92.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize 'yake_keywords' column as empty string if it's not already a string\n",
    "df_cpc_y02_subclass['yake_keywords'] = df_cpc_y02_subclass['yake_keywords'].apply(lambda x: '' if pd.isnull(x) else x)\n",
    "\n",
    "for index, row in tqdm(df_cpc_y02_subclass.iterrows()):\n",
    "    # For each row in df_cpc_y02, check every row in df_cpc_y02_subclass\n",
    "    for _, sub_row in df_cpc_y02.iterrows():\n",
    "        # Check if 'cpc_classification' contains string in 'cpc_subclass' column\n",
    "        if row['cpc_subclass'] in sub_row['cpc_classification']:\n",
    "            # Convert the list of keywords to a string and append to 'yake_keywords' column\n",
    "            keywords_string = ', '.join(sub_row['yake_keywords_titles_pos'])\n",
    "            df_cpc_y02_subclass.at[index, 'yake_keywords'] += keywords_string + ', '\n",
    "\n",
    "# Cast cells of 'yake_keywords' column to lists\n",
    "df_cpc_y02_subclass['yake_keywords'] = df_cpc_y02_subclass['yake_keywords'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of all keywords\n",
    "keyword_list = sum(df_cpc_y02_subclass['yake_keywords'].tolist(), [])\n",
    "\n",
    "# Generate list of unique keywords\n",
    "unique_keywords = list(set(keyword_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:00<00:00, 203548.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract titles from nested lists in column 'yake_keywords_titles_lower'\n",
    "df_cpc_y02['yake_keywords_titles_lower_kw'] = df_cpc_y02['yake_keywords_titles_lower'].progress_apply(lambda x: [keyword[0] for keyword in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:02<00:00, 248.05it/s]\n",
      "507it [00:04, 114.73it/s]\n"
     ]
    }
   ],
   "source": [
    "df_keyword = pd.DataFrame(columns=['keyword', 'cpc_subclass'])\n",
    "\n",
    "# Set keyword to 'keyword' column\n",
    "df_keyword['keyword'] = unique_keywords\n",
    "\n",
    "# Get the confidence of the keyword\n",
    "def get_confidence(keyword):\n",
    "    for index, row in df_cpc_y02.iterrows():\n",
    "        for yake_keyword in row['yake_keywords_titles_lower']:\n",
    "            if yake_keyword[0] == keyword:\n",
    "                return yake_keyword[1]\n",
    "    return None  # return None if the keyword is not found\n",
    "\n",
    "df_keyword['yake_confidence'] = df_keyword['keyword'].progress_apply(get_confidence)\n",
    "\n",
    "# Initialize 'cpc_subclass' and 'cpc_classification' column as empty string if it's not already a string\n",
    "df_keyword['cpc_subclass'] = df_keyword['cpc_subclass'].apply(lambda x: '' if pd.isnull(x) else x)\n",
    "\n",
    "# Initialize 'cpc_classification' column\n",
    "df_keyword['cpc_classification'] = ''\n",
    "df_keyword['cpc_classification'] = df_keyword['cpc_classification'].apply(lambda x: '' if pd.isnull(x) else x)\n",
    "\n",
    "for index,row in tqdm(df_keyword.iterrows()):\n",
    "    # For each row in df_cpc_y02, check every row in df_cpc_y02_subclass\n",
    "    for _, sub_row in df_cpc_y02.iterrows():\n",
    "        # Check if 'yake_keywords' contains string in 'keyword' column\n",
    "        if row['keyword'] in sub_row['yake_keywords_titles_lower_kw']:\n",
    "            # Append 'cpc_subclass' and 'cpc_classification' to 'cpc_subclass' or 'cpc_classification' column\n",
    "            # if sub_row['cpc_subclass'] not in df_keyword.at[index, 'cpc_subclass']:\n",
    "            #     df_keyword.at[index, 'cpc_subclass'] += sub_row['cpc_subclass'] + ', '\n",
    "            if sub_row['cpc_classification'] not in df_keyword.at[index, 'cpc_classification']:\n",
    "                df_keyword.at[index, 'cpc_classification'] += sub_row['cpc_classification'] + ', '\n",
    "    # Remove trailing comma and whitespace\n",
    "    df_keyword.at[index, 'cpc_subclass'] = df_keyword.at[index, 'cpc_subclass'][:-2]\n",
    "    df_keyword.at[index, 'cpc_classification'] = df_keyword.at[index, 'cpc_classification'][:-2]\n",
    "\n",
    "# Cast cells of 'cpc_subclass' column to lists\n",
    "# df_keyword['cpc_subclass'] = df_keyword['cpc_subclass'].str.split(', ')\n",
    "df_keyword['cpc_classification'] = df_keyword['cpc_classification'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows where 'keyword' is empty\n",
    "df_keyword = df_keyword[df_keyword['keyword'] != '']\n",
    "# Reset index\n",
    "df_keyword.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all rows where n_gram is <=2 words - CURRENTLY NOT IMPLEMENTED\n",
    "# df_keyword = df_keyword[df_keyword['keyword'].str.count(' ') >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword_cpc_explode = df_keyword.explode('cpc_subclass')\n",
    "# Reset index\n",
    "df_keyword_cpc_explode = df_keyword_cpc_explode.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keyword_y02_classification_embeddings_processed.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = pd.read_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keyword_y02_classification_embeddings_processed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/anferico_bert-for-patents. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:04<00:00, 122.41it/s]\n",
      "100%|██████████| 506/506 [00:02<00:00, 239.74it/s]\n",
      "100%|██████████| 506/506 [00:06<00:00, 73.95it/s]\n"
     ]
    }
   ],
   "source": [
    "df_keyword['keyword_patentsberta_embedding'] = df_keyword['keyword'].progress_apply(lambda x: model_patentsberta.encode(x))\n",
    "df_keyword['keyword_climatebert_embedding'] = df_keyword['keyword'].progress_apply(lambda x: model_climatebert.encode(x))\n",
    "df_keyword['keyword_bertforpatents_embedding'] = df_keyword['keyword'].progress_apply(lambda x: model_bertforpatents.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [00:00<00:00, 941996.37it/s]\n"
     ]
    }
   ],
   "source": [
    "df_keyword['cpc_subclass'] = df_keyword['cpc_classification'].progress_apply(lambda x: x[-1][:4] if x and len(x[-1]) >= 4 else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openTSNE import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/test split\n",
    "patentsberta_x_train, patentsberta_x_test = train_test_split(df_keyword['keyword_patentsberta_embedding'].tolist(), test_size=0.2, random_state=42)\n",
    "patentsberta_x_train_np = np.array(patentsberta_x_train)\n",
    "patentsberta_x_test_np = np.array(patentsberta_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    random_state=42,\n",
    "    n_iter=1000,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE on training data\n",
    "patentsberta_embedding_train = tsne.fit(patentsberta_x_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patentsberta_embedding_test = patentsberta_embedding_train.transform(patentsberta_x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patentsberta_embedding = np.concatenate((patentsberta_embedding_train, patentsberta_embedding_test), axis=0)\n",
    "df_keyword['keyword_patentsberta_embedding_tsne'] = patentsberta_embedding.tolist()\n",
    "df_keyword['keyword_patentsberta_embedding_tsne_x'] = df_keyword['keyword_patentsberta_embedding_tsne'].apply(lambda x: x[0])\n",
    "df_keyword['keyword_patentsberta_embedding_tsne_y'] = df_keyword['keyword_patentsberta_embedding_tsne'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = pd.read_json('/mnt/hdd01/patentsview/CPC Classification/df_keyword_y02_postprocessed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_patentsberta = px.scatter(df_keyword, x=\"keyword_bertforpatents_embedding_tsne_x\", y=\"keyword_bertforpatents_embedding_tsne_y\", hover_name=\"keyword\", color=\"solar\", title=\"PatentsBERTa t-SNE\")\n",
    "fig_patentsberta.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
