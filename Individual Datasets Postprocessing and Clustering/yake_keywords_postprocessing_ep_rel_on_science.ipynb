{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_yake_science_ep = pd.read_json('/mnt/hdd01/PATSTAT Working Directory/Reliance on Science/cleantech_epo_rel_on_science_abstract_yake.json')\n",
    "df_yake_science_uspto = pd.read_json('/mnt/hdd01/patentsview/Reliance on Science - Cleantech Patents/df_oaid_Cleantech_Y02_individual_works_yake.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102980/102980 [00:00<00:00, 711865.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete duplicates from lists in 'publn_nr' column\n",
    "df_yake_science_ep['publn_nr'] = df_yake_science_ep['publn_nr'].progress_apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>display_name</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>type</th>\n",
       "      <th>cited_by_count</th>\n",
       "      <th>is_retracted</th>\n",
       "      <th>is_paratext</th>\n",
       "      <th>cited_by_api_url</th>\n",
       "      <th>abstract_inverted_index</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_embedding</th>\n",
       "      <th>oaid</th>\n",
       "      <th>publn_nr</th>\n",
       "      <th>full_oaid</th>\n",
       "      <th>keywords_yake_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openalex.org/W2092669321</td>\n",
       "      <td>https://doi.org/10.1099/00222615-48-1-89</td>\n",
       "      <td>Oligofructose contributes to the protective ro...</td>\n",
       "      <td>Oligofructose contributes to the protective ro...</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999-01-01</td>\n",
       "      <td>article</td>\n",
       "      <td>66</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.openalex.org/works?filter=cites:W2...</td>\n",
       "      <td>{'IndexLength': 281, 'InvertedIndex': {'Bifido...</td>\n",
       "      <td>Bifidobacteria are dominant in the gut of full...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2092669321</td>\n",
       "      <td>[], 6, 0, 3, 5, 1, ', 2, []</td>\n",
       "      <td>https://openalex.org/W2092669321</td>\n",
       "      <td>[[[gut of full-term, 0.036202461000000005], [f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://openalex.org/W2169568623</td>\n",
       "      <td>https://doi.org/10.1093/glycob/10.11.1193</td>\n",
       "      <td>Lactobacillus johnsonii La1 shares carbohydrat...</td>\n",
       "      <td>Lactobacillus johnsonii La1 shares carbohydrat...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-11-01</td>\n",
       "      <td>article</td>\n",
       "      <td>149</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.openalex.org/works?filter=cites:W2...</td>\n",
       "      <td>{'IndexLength': 144, 'InvertedIndex': {'The': ...</td>\n",
       "      <td>The carbohydrate-binding specificities of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2169568623</td>\n",
       "      <td>[], 6, 7, 3, 1, ', 2, [, 9]</td>\n",
       "      <td>https://openalex.org/W2169568623</td>\n",
       "      <td>[[[lactic acid bacterium, 0.005116579200000000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://openalex.org/W2041158027</td>\n",
       "      <td>https://doi.org/10.1016/0378-4363(85)90593-5</td>\n",
       "      <td>Measurement of diffusion length and surface re...</td>\n",
       "      <td>Measurement of diffusion length and surface re...</td>\n",
       "      <td>1985</td>\n",
       "      <td>1985-03-01</td>\n",
       "      <td>article</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.openalex.org/works?filter=cites:W2...</td>\n",
       "      <td>{'IndexLength': 67, 'InvertedIndex': {'A': [0]...</td>\n",
       "      <td>A method is proposed for measuring the diffusi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2041158027</td>\n",
       "      <td>[ , ], 6, 3, 8, ', ,, 2, [, 9]</td>\n",
       "      <td>https://openalex.org/W2041158027</td>\n",
       "      <td>[[[versus the inverse, 0.0], [Interdigitated B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://openalex.org/W1979799857</td>\n",
       "      <td>https://doi.org/10.1016/0003-9861(57)90491-5</td>\n",
       "      <td>Coupled nucleoside phosphorylase reactions in ...</td>\n",
       "      <td>Coupled nucleoside phosphorylase reactions in ...</td>\n",
       "      <td>1957</td>\n",
       "      <td>1957-01-01</td>\n",
       "      <td>article</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.openalex.org/works?filter=cites:W1...</td>\n",
       "      <td>{'IndexLength': 182, 'InvertedIndex': {'This':...</td>\n",
       "      <td>This chapter focuses on purine nucleoside phos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979799857</td>\n",
       "      <td>[ , ], 0, 3, 1, ', [, ,, 2, 4, 9]</td>\n",
       "      <td>https://openalex.org/W1979799857</td>\n",
       "      <td>[[[identified and studied, 0.0], [double the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://openalex.org/W2005739462</td>\n",
       "      <td>https://doi.org/10.1063/1.95435</td>\n",
       "      <td>Amorphous silicon solar cells fabricated by ph...</td>\n",
       "      <td>Amorphous silicon solar cells fabricated by ph...</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984-10-15</td>\n",
       "      <td>article</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.openalex.org/works?filter=cites:W2...</td>\n",
       "      <td>{'IndexLength': 129, 'InvertedIndex': {'Hydrog...</td>\n",
       "      <td>Hydrogenated amorphous silicon solar cells wer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005739462</td>\n",
       "      <td>[ , ], 6, 0, 5, 3, 1, 8, ', ,, 4, 2, [, 9]</td>\n",
       "      <td>https://openalex.org/W2005739462</td>\n",
       "      <td>[[[photochemical vapor deposition, 0.005377254...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  https://openalex.org/W2092669321   \n",
       "1  https://openalex.org/W2169568623   \n",
       "2  https://openalex.org/W2041158027   \n",
       "3  https://openalex.org/W1979799857   \n",
       "4  https://openalex.org/W2005739462   \n",
       "\n",
       "                                            doi  \\\n",
       "0      https://doi.org/10.1099/00222615-48-1-89   \n",
       "1     https://doi.org/10.1093/glycob/10.11.1193   \n",
       "2  https://doi.org/10.1016/0378-4363(85)90593-5   \n",
       "3  https://doi.org/10.1016/0003-9861(57)90491-5   \n",
       "4               https://doi.org/10.1063/1.95435   \n",
       "\n",
       "                                               title  \\\n",
       "0  Oligofructose contributes to the protective ro...   \n",
       "1  Lactobacillus johnsonii La1 shares carbohydrat...   \n",
       "2  Measurement of diffusion length and surface re...   \n",
       "3  Coupled nucleoside phosphorylase reactions in ...   \n",
       "4  Amorphous silicon solar cells fabricated by ph...   \n",
       "\n",
       "                                        display_name  publication_year  \\\n",
       "0  Oligofructose contributes to the protective ro...              1999   \n",
       "1  Lactobacillus johnsonii La1 shares carbohydrat...              2000   \n",
       "2  Measurement of diffusion length and surface re...              1985   \n",
       "3  Coupled nucleoside phosphorylase reactions in ...              1957   \n",
       "4  Amorphous silicon solar cells fabricated by ph...              1984   \n",
       "\n",
       "  publication_date     type  cited_by_count  is_retracted  is_paratext  \\\n",
       "0       1999-01-01  article              66         False        False   \n",
       "1       2000-11-01  article             149         False        False   \n",
       "2       1985-03-01  article               0         False        False   \n",
       "3       1957-01-01  article              20         False        False   \n",
       "4       1984-10-15  article              20         False        False   \n",
       "\n",
       "                                    cited_by_api_url  \\\n",
       "0  https://api.openalex.org/works?filter=cites:W2...   \n",
       "1  https://api.openalex.org/works?filter=cites:W2...   \n",
       "2  https://api.openalex.org/works?filter=cites:W2...   \n",
       "3  https://api.openalex.org/works?filter=cites:W1...   \n",
       "4  https://api.openalex.org/works?filter=cites:W2...   \n",
       "\n",
       "                             abstract_inverted_index  \\\n",
       "0  {'IndexLength': 281, 'InvertedIndex': {'Bifido...   \n",
       "1  {'IndexLength': 144, 'InvertedIndex': {'The': ...   \n",
       "2  {'IndexLength': 67, 'InvertedIndex': {'A': [0]...   \n",
       "3  {'IndexLength': 182, 'InvertedIndex': {'This':...   \n",
       "4  {'IndexLength': 129, 'InvertedIndex': {'Hydrog...   \n",
       "\n",
       "                                            abstract  abstract_embedding  \\\n",
       "0  Bifidobacteria are dominant in the gut of full...                 NaN   \n",
       "1  The carbohydrate-binding specificities of the ...                 NaN   \n",
       "2  A method is proposed for measuring the diffusi...                 NaN   \n",
       "3  This chapter focuses on purine nucleoside phos...                 NaN   \n",
       "4  Hydrogenated amorphous silicon solar cells wer...                 NaN   \n",
       "\n",
       "         oaid                                    publn_nr  \\\n",
       "0  2092669321                 [], 6, 0, 3, 5, 1, ', 2, []   \n",
       "1  2169568623                 [], 6, 7, 3, 1, ', 2, [, 9]   \n",
       "2  2041158027              [ , ], 6, 3, 8, ', ,, 2, [, 9]   \n",
       "3  1979799857           [ , ], 0, 3, 1, ', [, ,, 2, 4, 9]   \n",
       "4  2005739462  [ , ], 6, 0, 5, 3, 1, 8, ', ,, 4, 2, [, 9]   \n",
       "\n",
       "                          full_oaid  \\\n",
       "0  https://openalex.org/W2092669321   \n",
       "1  https://openalex.org/W2169568623   \n",
       "2  https://openalex.org/W2041158027   \n",
       "3  https://openalex.org/W1979799857   \n",
       "4  https://openalex.org/W2005739462   \n",
       "\n",
       "                              keywords_yake_abstract  \n",
       "0  [[[gut of full-term, 0.036202461000000005], [f...  \n",
       "1  [[[lactic acid bacterium, 0.005116579200000000...  \n",
       "2  [[[versus the inverse, 0.0], [Interdigitated B...  \n",
       "3  [[[identified and studied, 0.0], [double the o...  \n",
       "4  [[[photochemical vapor deposition, 0.005377254...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yake_science_ep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_science_ep.iterrows()):\n",
    "    # Cast row 'keywords_yake_abstract' column to string\n",
    "    row['keywords_yake_abstract'] = str(row['keywords_yake_abstract']).lower()\n",
    "    # Check if keywords_yake_abstract column starts with \"[[[\" and ends with \"]]]\"\"\n",
    "    if row['keywords_yake_abstract'].startswith('[[[') and row['keywords_yake_abstract'].endswith(']]]'):\n",
    "        # Remove first \"[\" and last \"]\" from keywords_yake_abstract column\n",
    "        row['keywords_yake_abstract'] = row['keywords_yake_abstract'][1:-1]\n",
    "    # Cast row 'keywords_yake_abstract' column to list\n",
    "    row['keywords_yake_abstract'] = ast.literal_eval(row['keywords_yake_abstract'])\n",
    "    # Assign modified 'keywords_yake_abstract' list to temporary variable\n",
    "    keywords_yake_temp = row['keywords_yake_abstract']\n",
    "    # Assign temporary variable to 'keywords_yake_abstract' column\n",
    "    df_yake_science_ep.at[index, 'keywords_yake_abstract'] = keywords_yake_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_science_uspto.iterrows()):\n",
    "    # Cast row 'keywords_yake' column to string\n",
    "    row['keywords_yake'] = str(row['keywords_yake']).lower()\n",
    "    # Check if keywords_yake column starts with \"[[[\" and ends with \"]]]\"\"\n",
    "    if row['keywords_yake'].startswith('[[[') and row['keywords_yake'].endswith(']]]'):\n",
    "        # Remove first \"[\" and last \"]\" from keywords_yake column\n",
    "        row['keywords_yake'] = row['keywords_yake'][1:-1]\n",
    "    # Cast row 'keywords_yake' column to list\n",
    "    row['keywords_yake'] = ast.literal_eval(row['keywords_yake'])\n",
    "    # Assign modified 'keywords_yake' list to temporary variable\n",
    "    keywords_yake_temp = row['keywords_yake']\n",
    "    # Assign temporary variable to 'keywords_yake' column\n",
    "    df_yake_science_uspto.at[index, 'keywords_yake'] = keywords_yake_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yake_science_ep['keywords_yake_abstract_exploded'] = ''\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_science_ep.iterrows()):\n",
    "    # Check if 'keywords_yake_abstract' column is not a list\n",
    "    if not isinstance(row['keywords_yake_abstract'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake_abstract' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_abstract']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake_abstract' column and append to 'keywords_yake_abstract_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_abstract']:\n",
    "            df_yake_science_ep.at[index, 'keywords_yake_abstract_exploded'] += keyword[0] + ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yake_science_uspto['keywords_yake_exploded'] = ''\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_science_uspto.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['keywords_yake'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append to 'keywords_yake_abstract_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake']:\n",
    "            df_yake_science_uspto.at[index, 'keywords_yake_exploded'] += keyword[0] + ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'keywords_yake_abstract_exploded' column to list\n",
    "df_yake_science_ep['keywords_yake_abstract_exploded'] = df_yake_science_ep['keywords_yake_abstract_exploded'].str.split(', ')\n",
    "df_yake_science_uspto['keywords_yake_exploded'] = df_yake_science_uspto['keywords_yake_exploded'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete last item in 'keywords_yake_abstract_exploded' cells if empty string\n",
    "df_yake_science_ep['keywords_yake_abstract_exploded'] = df_yake_science_ep['keywords_yake_abstract_exploded'].progress_apply(lambda x: x[:-1] if x and x[-1] == '' else x)\n",
    "df_yake_science_uspto['keywords_yake_exploded'] = df_yake_science_uspto['keywords_yake_exploded'].progress_apply(lambda x: x[:-1] if x and x[-1] == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_yake_science_ep and df_yake_science_uspto dataframes\n",
    "df_yake_science = pd.merge(df_yake_science_ep, df_yake_science_uspto, how='outer', on='oaid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = []\n",
    "min_yake_conf = 0.05\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_science.iterrows()):\n",
    "    # Check if 'keywords_yake_abstract' column is not a list\n",
    "    if not isinstance(row['keywords_yake_abstract'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake_abstract' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_abstract']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake_abstract' column and append to 'keywords_yake_abstract_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_abstract']:\n",
    "            if keyword[1] <= min_yake_conf:\n",
    "                keywords_list.append(keyword[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider alphanumeric characters in keywords\n",
    "keywords_list = [\n",
    "    keyword for keyword in keywords_list \n",
    "    if all(word.isalnum() for word in keyword.split())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique list of keywords\n",
    "keywords_list_unique = list(set(keywords_list))\n",
    "df_keywords_list_unique = pd.DataFrame(keywords_list_unique, columns=['keyword_yake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count absolute frequency of each keyword\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Flatten the list of keywords into a single list\n",
    "flattened_keywords = [keyword for keyword_list in tqdm(df_yake_science['keywords_yake_abstract_exploded']) for keyword in keyword_list]\n",
    "\n",
    "# Step 2: Use collections.Counter to count the occurrences of each keyword\n",
    "keyword_counts = Counter(flattened_keywords)\n",
    "\n",
    "# Step 3: Convert the keyword counts to a DataFrame\n",
    "count_df = pd.DataFrame(keyword_counts.items(), columns=['keyword_yake', 'abs_frequency'])\n",
    "\n",
    "# Step 4: Merge the count_df with df_keywords_list_unique on the 'keyword_yake' column\n",
    "df_keywords_list_unique = df_keywords_list_unique.merge(count_df, on='keyword_yake', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAYBE I NEED TO REDO THIS WITH FULL TEXT CLAIMS INSTEAD OF JUST THE YAKE KEYWORDS\n",
    "\n",
    "# Calculate document frequency by dividing absolute frequency by len of df_yake_science\n",
    "# df_keywords_list_unique['doc_frequency'] = df_keywords_list_unique['abs_frequency'] / len(df_yake_science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune dataframe by document frequency and absolute frequency\n",
    "min_abs_frequency = 5\n",
    "max_abs_frequency = 1000\n",
    "# max_doc_frequency = 0.3\n",
    "\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique[(df_keywords_list_unique['abs_frequency'] >= min_abs_frequency) & (df_keywords_list_unique['abs_frequency'] <= max_abs_frequency)]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider n-grams with n >= 2 - NOT CONSIDERED FOR NOW\n",
    "# min_ngram_length = 2\n",
    "\n",
    "# # Generate copy of df_keywords_list_unique_pruned\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# def filter_keywords(keyword_string):\n",
    "#     # Split the keyword string into individual keywords\n",
    "#     keywords = keyword_string.split(', ')\n",
    "#     # Filter out keywords with fewer than 2 words\n",
    "#     filtered_keywords = [keyword for keyword in keywords if len(keyword.split()) >= min_ngram_length]\n",
    "#     # Join the filtered keywords back into a string\n",
    "#     return ', '.join(filtered_keywords)\n",
    "\n",
    "# # Create a copy of df_keywords_list_unique_pruned to avoid SettingWithCopyWarning\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# # Now use .loc to modify the 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(filter_keywords)\n",
    "\n",
    "# # Delete rows with empty 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# # Reset index\n",
    "# df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Generate copy of df_keywords_list_unique_pruned\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Delete all stopwords from 'keyword_yake' column\n",
    "df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(\n",
    "    lambda x: '' if x in stopwords.words('english') else x\n",
    ")\n",
    "\n",
    "# Remove empty rows\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAYBE I NEED TO REDO THIS PART WITH THE CLAIM TEXTS INSTEAD OF THE KEYWORDS - NOT CONSIDERED FOR NOW\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Download spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Perform part-of-speech tagging on the 'keyword_yake' column \n",
    "# and save the POS tags in a new column 'keyword_yake_pos'\n",
    "df_keywords_list_unique_pruned_pos['keyword_yake_pos'] = df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(\n",
    "    lambda x: [token.pos_ for token in nlp(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all keywords that do not contain NOUN, PRON or PROPN in their POS tags\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned_pos[df_keywords_list_unique_pruned_pos['keyword_yake_pos'].progress_apply(\n",
    "    lambda x: any(pos in ['NOUN', 'PRON', 'PROPN'] for pos in x)\n",
    ")]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned_pos.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT CONSIDERED FOR NOW\n",
    "# import re\n",
    "\n",
    "# # Function to check if a word contains at least three consecutive capital letters (filter out all abbreviations)\n",
    "# def has_three_consecutive_caps(word):\n",
    "#     return re.search(r'[A-Z]{3,}', word) is not None\n",
    "\n",
    "# # Function to check if any word in an n-gram contains at least three consecutive capital letters\n",
    "# def check_ngram(ngram):\n",
    "#     return any(has_three_consecutive_caps(word) for word in ngram.split())\n",
    "\n",
    "# # Use the DataFrame `apply` method to filter rows\n",
    "# df_keywords_list_unique_pruned_pos_filtered = df_keywords_list_unique_pruned_pos[~df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(check_ngram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT CONSIDERED FOR NOW\n",
    "# # Test to filter out brown corpus words\n",
    "# from nltk.corpus import brown\n",
    "# # nltk.download('brown')\n",
    "\n",
    "# # Step 1: Pre-calculate the set of Brown corpus words\n",
    "# brown_words_set = set(brown.words())\n",
    "\n",
    "# # Function to check if any word in a keyword phrase exists in the Brown corpus\n",
    "# def check_brown_words(phrase):\n",
    "#     return any(word in brown_words_set for word in phrase.split())\n",
    "\n",
    "# # Step 2: Use progress_apply with the optimized function\n",
    "# df_keywords_list_unique_pruned_pos_filtered_brown = df_keywords_list_unique_pruned_pos_filtered[\n",
    "#     df_keywords_list_unique_pruned_pos_filtered['keyword_yake'].progress_apply(check_brown_words)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Keyword Dataframe and Patent Ids and CPC - PatentsView or aggregate Keywords - EPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "# for index, row in tqdm(df_yake_science_cpc.iterrows()):\n",
    "for index,row in tqdm(df_yake_science.iterrows()):\n",
    "    if row['keywords_yake_abstract_exploded'] == []:\n",
    "        continue\n",
    "    if not any(keyword for keyword in row['keywords_yake_abstract_exploded']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append patent_id and cpc information\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_abstract_exploded']:\n",
    "            # EPO\n",
    "            rows_list.append({'publn_nr': row['publn_nr'],\n",
    "                              'cpc_class_symbol': row['cpc_class_symbol'],\n",
    "                              'keyword_yake_claims': keyword})\n",
    "\n",
    "df_claims_keywords = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate df_claims_keywords on 'keyword_yake_claims' column\n",
    "df_claims_keywords_agg = df_claims_keywords.groupby('keyword_yake_claims').agg(\n",
    "    publn_nr_list = pd.NamedAgg(column='publn_nr', aggfunc=list),\n",
    "    cpc_class_symbol_list = pd.NamedAgg(column='cpc_class_symbol', aggfunc=list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from 'cpc_class_symbol_list'\n",
    "df_claims_keywords_agg['cpc_class_symbol_list'] = df_claims_keywords_agg['cpc_class_symbol_list'].progress_apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_claims_keywords_agg with df_keywords_list_unique_embeddings on 'keyword_yake_claims' column and generate new dataframe\n",
    "df_claims_keywords_list = pd.merge(df_keywords_list_unique_pruned_pos, df_claims_keywords_agg, left_on='keyword_yake', right_on='keyword_yake_claims', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate copy of df_claims_keywords_list\n",
    "df_keywords_list_unique_embeddings = df_claims_keywords_list.copy()\n",
    "\n",
    "# Perform sentence embedding on the 'keyword_yake' (PatentsView) or 'keywords_yake_abstract' (EPO) column\n",
    "df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_patentsberta.encode(x)\n",
    ")\n",
    "\n",
    "df_keywords_list_unique_embeddings['keyword_yake_climatebert_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_climatebert.encode(x)\n",
    ")\n",
    "\n",
    "df_keywords_list_unique_embeddings['keyword_yake_bertforpatents_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_bertforpatents.encode(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to json - Make sure to use correct directory (PatentsView vs. EPO)\n",
    "df_keywords_list_unique_embeddings.to_json('/mnt/hdd01/PATSTAT Working Directory/PATSTAT/cleantech_epo_text_data_pivot_cleaned_yake_embeddings_processed.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keywords_y02_ep_embeddings_processed.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
