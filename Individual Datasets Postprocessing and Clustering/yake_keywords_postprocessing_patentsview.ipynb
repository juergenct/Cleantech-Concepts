{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_yake_claims = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [01:11, 7163.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Cast row 'keywords_yake' column to string\n",
    "    row['keywords_yake'] = str(row['keywords_yake']).lower()\n",
    "    # Check if keywords_yake column starts with \"[[[\" and ends with \"]]]\"\"\n",
    "    if row['keywords_yake'].startswith('[[[') and row['keywords_yake'].endswith(']]]'):\n",
    "        # Remove first \"[\" and last \"]\" from keywords_yake column\n",
    "        row['keywords_yake'] = row['keywords_yake'][1:-1]\n",
    "    # Cast row 'keywords_yake' column to list\n",
    "    row['keywords_yake'] = ast.literal_eval(row['keywords_yake'])\n",
    "    # Assign modified 'keywords_yake' list to temporary variable\n",
    "    keywords_yake_temp = row['keywords_yake']\n",
    "    # Assign temporary variable to 'keywords_yake' column\n",
    "    df_yake_claims.at[index, 'keywords_yake'] = keywords_yake_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [02:06, 4070.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df_yake_claims['keywords_yake_exploded'] = ''\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['keywords_yake'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append to 'keywords_yake_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake']:\n",
    "            df_yake_claims.at[index, 'keywords_yake_exploded'] += keyword[0] + ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'keywords_yake_exploded' column to list\n",
    "df_yake_claims['keywords_yake_exploded'] = df_yake_claims['keywords_yake_exploded'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515742/515742 [00:00<00:00, 1495513.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete last item in 'keywords_yake_exploded' cells if empty string\n",
    "df_yake_claims['keywords_yake_exploded'] = df_yake_claims['keywords_yake_exploded'].progress_apply(lambda x: x[:-1] if x and x[-1] == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [00:16, 31662.42it/s]\n"
     ]
    }
   ],
   "source": [
    "keywords_list = []\n",
    "min_yake_conf = 0.05\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Check if 'keywords_yake' column is not a list\n",
    "    if not isinstance(row['keywords_yake'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append to 'keywords_yake_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake']:\n",
    "            if keyword[1] <= min_yake_conf:\n",
    "                keywords_list.append(keyword[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider alphanumeric characters in keywords\n",
    "keywords_list = [\n",
    "    keyword for keyword in keywords_list \n",
    "    if all(word.isalnum() for word in keyword.split())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique list of keywords\n",
    "keywords_list_unique = list(set(keywords_list))\n",
    "df_keywords_list_unique = pd.DataFrame(keywords_list_unique, columns=['keyword_yake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 515742/515742 [00:00<00:00, 2761819.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count absolute frequency of each keyword\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Flatten the list of keywords into a single list\n",
    "flattened_keywords = [keyword for keyword_list in tqdm(df_yake_claims['keywords_yake_exploded']) for keyword in keyword_list]\n",
    "\n",
    "# Step 2: Use collections.Counter to count the occurrences of each keyword\n",
    "keyword_counts = Counter(flattened_keywords)\n",
    "\n",
    "# Step 3: Convert the keyword counts to a DataFrame\n",
    "count_df = pd.DataFrame(keyword_counts.items(), columns=['keyword_yake', 'abs_frequency'])\n",
    "\n",
    "# Step 4: Merge the count_df with df_keywords_list_unique on the 'keyword_yake' column\n",
    "df_keywords_list_unique = df_keywords_list_unique.merge(count_df, on='keyword_yake', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAYBE I NEED TO REDO THIS WITH FULL TEXT CLAIMS INSTEAD OF JUST THE YAKE KEYWORDS\n",
    "\n",
    "# Calculate document frequency by dividing absolute frequency by len of df_yake_claims\n",
    "# df_keywords_list_unique['doc_frequency'] = df_keywords_list_unique['abs_frequency'] / len(df_yake_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune dataframe by document frequency and absolute frequency\n",
    "min_abs_frequency = 5\n",
    "max_abs_frequency = 1000\n",
    "# max_doc_frequency = 0.3\n",
    "\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique[(df_keywords_list_unique['abs_frequency'] >= min_abs_frequency) & (df_keywords_list_unique['abs_frequency'] <= max_abs_frequency)]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider n-grams with n >= 2 - NOT CONSIDERED FOR NOW\n",
    "# min_ngram_length = 2\n",
    "\n",
    "# # Generate copy of df_keywords_list_unique_pruned\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# def filter_keywords(keyword_string):\n",
    "#     # Split the keyword string into individual keywords\n",
    "#     keywords = keyword_string.split(', ')\n",
    "#     # Filter out keywords with fewer than 2 words\n",
    "#     filtered_keywords = [keyword for keyword in keywords if len(keyword.split()) >= min_ngram_length]\n",
    "#     # Join the filtered keywords back into a string\n",
    "#     return ', '.join(filtered_keywords)\n",
    "\n",
    "# # Create a copy of df_keywords_list_unique_pruned to avoid SettingWithCopyWarning\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# # Now use .loc to modify the 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(filter_keywords)\n",
    "\n",
    "# # Delete rows with empty 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# # Reset index\n",
    "# df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69546/69546 [00:03<00:00, 19472.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Generate copy of df_keywords_list_unique_pruned\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Delete all stopwords from 'keyword_yake' column\n",
    "df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(\n",
    "    lambda x: '' if x in stopwords.words('english') else x\n",
    ")\n",
    "\n",
    "# Remove empty rows\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 69546/69546 [02:46<00:00, 418.77it/s]\n"
     ]
    }
   ],
   "source": [
    "### MAYBE I NEED TO REDO THIS PART WITH THE CLAIM TEXTS INSTEAD OF THE KEYWORDS - NOT CONSIDERED FOR NOW\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Download spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Perform part-of-speech tagging on the 'keyword_yake' column \n",
    "# and save the POS tags in a new column 'keyword_yake_pos'\n",
    "df_keywords_list_unique_pruned_pos['keyword_yake_pos'] = df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(\n",
    "    lambda x: [token.pos_ for token in nlp(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69546/69546 [00:00<00:00, 918644.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filter out all keywords that do not contain NOUN, PRON or PROPN in their POS tags\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned_pos[df_keywords_list_unique_pruned_pos['keyword_yake_pos'].progress_apply(\n",
    "    lambda x: any(pos in ['NOUN', 'PRON', 'PROPN'] for pos in x)\n",
    ")]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned_pos.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT CONSIDERED FOR NOW\n",
    "# import re\n",
    "\n",
    "# # Function to check if a word contains at least three consecutive capital letters (filter out all abbreviations)\n",
    "# def has_three_consecutive_caps(word):\n",
    "#     return re.search(r'[A-Z]{3,}', word) is not None\n",
    "\n",
    "# # Function to check if any word in an n-gram contains at least three consecutive capital letters\n",
    "# def check_ngram(ngram):\n",
    "#     return any(has_three_consecutive_caps(word) for word in ngram.split())\n",
    "\n",
    "# # Use the DataFrame `apply` method to filter rows\n",
    "# df_keywords_list_unique_pruned_pos_filtered = df_keywords_list_unique_pruned_pos[~df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(check_ngram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT CONSIDERED FOR NOW\n",
    "# # Test to filter out brown corpus words\n",
    "# from nltk.corpus import brown\n",
    "# # nltk.download('brown')\n",
    "\n",
    "# # Step 1: Pre-calculate the set of Brown corpus words\n",
    "# brown_words_set = set(brown.words())\n",
    "\n",
    "# # Function to check if any word in a keyword phrase exists in the Brown corpus\n",
    "# def check_brown_words(phrase):\n",
    "#     return any(word in brown_words_set for word in phrase.split())\n",
    "\n",
    "# # Step 2: Use progress_apply with the optimized function\n",
    "# df_keywords_list_unique_pruned_pos_filtered_brown = df_keywords_list_unique_pruned_pos_filtered[\n",
    "#     df_keywords_list_unique_pruned_pos_filtered['keyword_yake'].progress_apply(check_brown_words)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Keyword Dataframe and Patent Ids and CPC Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PatentsView\n",
    "df_cpc = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/df_patentsview_patent_cpc_grouped_cleantech.json')\n",
    "# Merge df_yake_claims with df_cpc only keep 'cpc' from df_cpc\n",
    "df_yake_claims_cpc = df_yake_claims.merge(df_cpc[['patent_id', 'cpc']], on='patent_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515742it [00:35, 14416.43it/s]\n"
     ]
    }
   ],
   "source": [
    "rows_list = []\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_claims_cpc.iterrows()):\n",
    "    if row['keywords_yake_exploded'] == []:\n",
    "        continue\n",
    "    if not any(keyword for keyword in row['keywords_yake_exploded']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append patent_id and cpc information\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_exploded']:\n",
    "            # PatentsView\n",
    "            rows_list.append({'patent_id': row['patent_id'], \n",
    "                              'cpc': row['cpc'], \n",
    "                              'keyword_yake': keyword})\n",
    "\n",
    "df_cpc_keywords = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5152903/5152903 [00:12<00:00, 427204.78it/s] \n",
      "100%|██████████| 5152903/5152903 [00:07<00:00, 646220.31it/s] \n",
      "/tmp/ipykernel_12815/2403604602.py:23: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_cpc_keywords_agg[['cpc_subclass_list', 'cpc_group_list']] = df_cpc_keywords_agg[['cpc_subclass_list', 'cpc_group_list']].applymap(flatten_nested_list)\n"
     ]
    }
   ],
   "source": [
    "# Extract 'cpc_subclass' into a new column\n",
    "df_cpc_keywords['cpc_subclass'] = df_cpc_keywords['cpc'].progress_apply(\n",
    "    lambda x: [entry['cpc_subclass'] for entry in x.values() if 'cpc_subclass' in entry]\n",
    ")\n",
    "\n",
    "# Extract 'cpc_group' into a new column\n",
    "df_cpc_keywords['cpc_group'] = df_cpc_keywords['cpc'].progress_apply(\n",
    "    lambda x: [entry['cpc_group'] for entry in x.values() if 'cpc_group' in entry]\n",
    ")\n",
    "\n",
    "# Aggregate df_cpc_keywords on 'keyword_yake' column\n",
    "df_cpc_keywords_agg = df_cpc_keywords.groupby('keyword_yake').agg(\n",
    "    patent_id_list = pd.NamedAgg(column='patent_id', aggfunc=list),\n",
    "    cpc_subclass_list = pd.NamedAgg(column='cpc_subclass', aggfunc=list),\n",
    "    cpc_group_list = pd.NamedAgg(column='cpc_group', aggfunc=list)\n",
    ")\n",
    "\n",
    "# Define a function to flatten nested lists\n",
    "def flatten_nested_list(nested_list):\n",
    "    return [item for sublist in nested_list for item in sublist]\n",
    "\n",
    "# Convert nested lists in 'cpc_subclass_list' and 'cpc_group_list' columns to lists of strings\n",
    "df_cpc_keywords_agg[['cpc_subclass_list', 'cpc_group_list']] = df_cpc_keywords_agg[['cpc_subclass_list', 'cpc_group_list']].applymap(flatten_nested_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1321218/1321218 [00:01<00:00, 935508.75it/s]\n",
      "100%|██████████| 1321218/1321218 [00:07<00:00, 184486.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates from 'cpc_subclass_list'\n",
    "df_cpc_keywords_agg['cpc_subclass_list'] = df_cpc_keywords_agg['cpc_subclass_list'].progress_apply(lambda x: list(set(x)))\n",
    "\n",
    "# # Remove duplicates from 'cpc_group_list'\n",
    "df_cpc_keywords_agg['cpc_group_list'] = df_cpc_keywords_agg['cpc_group_list'].progress_apply(lambda x: list(set(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_cpc_keywords_agg with df_keywords_list_unique_pruned_pos on 'keyword_yake' column and generate new dataframe\n",
    "df_cpc_keywords_list = pd.merge(df_keywords_list_unique_pruned_pos, df_cpc_keywords_agg, on='keyword_yake', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/anferico_bert-for-patents. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61731 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61731/61731 [06:44<00:00, 152.43it/s]\n",
      "100%|██████████| 61731/61731 [03:57<00:00, 259.42it/s]\n",
      "100%|██████████| 61731/61731 [13:04<00:00, 78.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate copy of df_cpc_keywords_list\n",
    "df_keywords_list_unique_embeddings = df_cpc_keywords_list.copy()\n",
    "\n",
    "# Perform sentence embedding on the 'keyword_yake' (PatentsView) or 'keywords_yake' (EPO) column\n",
    "df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_patentsberta.encode(x)\n",
    ")\n",
    "\n",
    "df_keywords_list_unique_embeddings['keyword_yake_climatebert_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_climatebert.encode(x)\n",
    ")\n",
    "\n",
    "df_keywords_list_unique_embeddings['keyword_yake_bertforpatents_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_bertforpatents.encode(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_yake</th>\n",
       "      <th>abs_frequency</th>\n",
       "      <th>keyword_yake_pos</th>\n",
       "      <th>patent_id_list</th>\n",
       "      <th>cpc_subclass_list</th>\n",
       "      <th>cpc_group_list</th>\n",
       "      <th>keyword_yake_patentsberta_embedding</th>\n",
       "      <th>keyword_yake_climatebert_embedding</th>\n",
       "      <th>keyword_yake_bertforpatents_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forming a source</td>\n",
       "      <td>5</td>\n",
       "      <td>[VERB, DET, NOUN]</td>\n",
       "      <td>[7026187, 7250333, 8586400, 9431243, 9709335]</td>\n",
       "      <td>[Y02E, Y02P]</td>\n",
       "      <td>[Y02E10/549, Y02E10/547, Y02P90/02, Y02E10/50,...</td>\n",
       "      <td>[-0.2973873, -0.37851676, -0.08203936, -0.0396...</td>\n",
       "      <td>[-0.07330753, -0.020932056, -0.04066288, -0.12...</td>\n",
       "      <td>[-0.25592694, 0.84284246, -0.15500082, 0.47458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>615</td>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>[10011900, 10032942, 10046978, 10066308, 10079...</td>\n",
       "      <td>[Y02C, Y02A, Y02W, Y02B, Y02T, Y02E, Y02P]</td>\n",
       "      <td>[Y02B80/10, Y02E10/52, Y02E60/50, Y02W10/37, Y...</td>\n",
       "      <td>[0.06464179, -0.63208365, -0.405623, 0.3495366...</td>\n",
       "      <td>[-0.056519262, 0.03321711, 0.013426696, 0.1743...</td>\n",
       "      <td>[-0.26295653, 0.042311374, -0.09004315, -0.316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>disconnecting the power</td>\n",
       "      <td>7</td>\n",
       "      <td>[VERB, DET, NOUN]</td>\n",
       "      <td>[5552680, 6081205, 6713989, 7990673, 8547074, ...</td>\n",
       "      <td>[Y02T, Y02E, Y02B, Y02D]</td>\n",
       "      <td>[Y02T90/16, Y02T90/14, Y02D10/00, Y02B20/00, Y...</td>\n",
       "      <td>[-0.23607938, -0.55960715, -0.33855042, -0.061...</td>\n",
       "      <td>[0.046874385, 0.06321991, 0.021172406, 0.05983...</td>\n",
       "      <td>[0.23535673, -0.3955746, -0.069739625, -0.6015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>catalyst regeneration zone</td>\n",
       "      <td>5</td>\n",
       "      <td>[PROPN, NOUN, NOUN]</td>\n",
       "      <td>[4071436, 4115250, 4115251, 4338788, 5519149]</td>\n",
       "      <td>[Y02T, Y02A, Y02E, Y02P]</td>\n",
       "      <td>[Y02P20/584, Y02E20/14, Y02T50/60, Y02A50/20]</td>\n",
       "      <td>[-0.30876154, -0.31838512, -0.133001, 0.132115...</td>\n",
       "      <td>[0.08325515, -0.014730735, 0.027471442, 0.1283...</td>\n",
       "      <td>[-0.19838068, -0.55406475, -0.80453604, -0.814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rigidity</td>\n",
       "      <td>21</td>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>[10014352, 10139130, 10164225, 10629794, 10777...</td>\n",
       "      <td>[Y02T, Y02E, Y02A, Y02P]</td>\n",
       "      <td>[Y02T10/62, Y02E10/72, Y02E10/549, Y02E60/50, ...</td>\n",
       "      <td>[0.12807769, -0.8062214, -0.28694445, 0.236054...</td>\n",
       "      <td>[0.0015207138, -0.012041591, 0.032385968, 0.00...</td>\n",
       "      <td>[-0.12907217, 0.72338533, -1.0261452, 0.242813...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 keyword_yake  abs_frequency     keyword_yake_pos  \\\n",
       "0            forming a source              5    [VERB, DET, NOUN]   \n",
       "1                    aluminum            615               [NOUN]   \n",
       "2     disconnecting the power              7    [VERB, DET, NOUN]   \n",
       "3  catalyst regeneration zone              5  [PROPN, NOUN, NOUN]   \n",
       "4                    rigidity             21               [NOUN]   \n",
       "\n",
       "                                      patent_id_list  \\\n",
       "0      [7026187, 7250333, 8586400, 9431243, 9709335]   \n",
       "1  [10011900, 10032942, 10046978, 10066308, 10079...   \n",
       "2  [5552680, 6081205, 6713989, 7990673, 8547074, ...   \n",
       "3      [4071436, 4115250, 4115251, 4338788, 5519149]   \n",
       "4  [10014352, 10139130, 10164225, 10629794, 10777...   \n",
       "\n",
       "                            cpc_subclass_list  \\\n",
       "0                                [Y02E, Y02P]   \n",
       "1  [Y02C, Y02A, Y02W, Y02B, Y02T, Y02E, Y02P]   \n",
       "2                    [Y02T, Y02E, Y02B, Y02D]   \n",
       "3                    [Y02T, Y02A, Y02E, Y02P]   \n",
       "4                    [Y02T, Y02E, Y02A, Y02P]   \n",
       "\n",
       "                                      cpc_group_list  \\\n",
       "0  [Y02E10/549, Y02E10/547, Y02P90/02, Y02E10/50,...   \n",
       "1  [Y02B80/10, Y02E10/52, Y02E60/50, Y02W10/37, Y...   \n",
       "2  [Y02T90/16, Y02T90/14, Y02D10/00, Y02B20/00, Y...   \n",
       "3      [Y02P20/584, Y02E20/14, Y02T50/60, Y02A50/20]   \n",
       "4  [Y02T10/62, Y02E10/72, Y02E10/549, Y02E60/50, ...   \n",
       "\n",
       "                 keyword_yake_patentsberta_embedding  \\\n",
       "0  [-0.2973873, -0.37851676, -0.08203936, -0.0396...   \n",
       "1  [0.06464179, -0.63208365, -0.405623, 0.3495366...   \n",
       "2  [-0.23607938, -0.55960715, -0.33855042, -0.061...   \n",
       "3  [-0.30876154, -0.31838512, -0.133001, 0.132115...   \n",
       "4  [0.12807769, -0.8062214, -0.28694445, 0.236054...   \n",
       "\n",
       "                  keyword_yake_climatebert_embedding  \\\n",
       "0  [-0.07330753, -0.020932056, -0.04066288, -0.12...   \n",
       "1  [-0.056519262, 0.03321711, 0.013426696, 0.1743...   \n",
       "2  [0.046874385, 0.06321991, 0.021172406, 0.05983...   \n",
       "3  [0.08325515, -0.014730735, 0.027471442, 0.1283...   \n",
       "4  [0.0015207138, -0.012041591, 0.032385968, 0.00...   \n",
       "\n",
       "               keyword_yake_bertforpatents_embedding  \n",
       "0  [-0.25592694, 0.84284246, -0.15500082, 0.47458...  \n",
       "1  [-0.26295653, 0.042311374, -0.09004315, -0.316...  \n",
       "2  [0.23535673, -0.3955746, -0.069739625, -0.6015...  \n",
       "3  [-0.19838068, -0.55406475, -0.80453604, -0.814...  \n",
       "4  [-0.12907217, 0.72338533, -1.0261452, 0.242813...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keywords_list_unique_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to json - Make sure to use correct directory (PatentsView vs. EPO)\n",
    "df_keywords_list_unique_embeddings.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keywords_y02_uspto_embeddings_processed.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data if necessary\n",
    "df_keywords_list_unique_embeddings = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "umap_dim = 2\n",
    "reducer = umap.UMAP(random_state=42, n_components=umap_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the embeddings into a 2D array\n",
    "embeddings_patentsberta = np.vstack(df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'].values)\n",
    "\n",
    "# Perform UMAP dimensionality reduction\n",
    "umap_embeddings_patentsberta = reducer.fit_transform(embeddings_patentsberta)\n",
    "\n",
    "# Assign the reduced dimension embeddings back to new DataFrame columns\n",
    "df_keywords_list_unique_embeddings_umap['keyword_yake_patentsberta_embedding_umap'] = list(umap_embeddings_patentsberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openTSNE import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/test split\n",
    "patentsberta_x_train, patentsberta_x_test = train_test_split(df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'].tolist(), test_size=0.2, random_state=42)\n",
    "patentsberta_x_train_np = np.array(patentsberta_x_train)\n",
    "patentsberta_x_test_np = np.array(patentsberta_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    random_state=42,\n",
    "    n_iter=1000,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE on training data\n",
    "patentsberta_embedding_train = tsne.fit(patentsberta_x_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patentsberta_embedding_test = patentsberta_embedding_train.transform(patentsberta_x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings_tsne = df_keywords_list_unique_embeddings.copy()\n",
    "\n",
    "patentsberta_embedding = np.concatenate((patentsberta_embedding_train, patentsberta_embedding_test), axis=0)\n",
    "df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne'] = patentsberta_embedding.tolist()\n",
    "df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne_x'] = df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne'].apply(lambda x: x[0])\n",
    "df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne_y'] = df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HDBSCAN\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_embeddings_hdbscan = df_keywords_list_unique_embeddings_tsne.copy()\n",
    "\n",
    "# Perform HDBSCAN clustering on the UMAP coordinates\n",
    "clusterer_patentsberta = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=1).fit(df_keywords_list_unique_embeddings_hdbscan[['keyword_yake_patentsberta_embedding_tsne_x', 'keyword_yake_patentsberta_embedding_tsne_y']])\n",
    "\n",
    "# Assign the cluster labels back to the DataFrame\n",
    "df_keywords_list_unique_embeddings_hdbscan['patentsberta_cluster'] = clusterer_patentsberta.labels_\n",
    "\n",
    "# Erase all rows with cluster -1\n",
    "df_keywords_list_unique_embeddings_hdbscan = df_keywords_list_unique_embeddings_hdbscan[df_keywords_list_unique_embeddings_hdbscan['patentsberta_cluster'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings_hdbscan.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings_tsne_hdbscan.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP embeddings in three figures, color coded by cluster\n",
    "fig_patentsberta = px.scatter(\n",
    "    df_keywords_list_unique_embeddings_hdbscan, \n",
    "    x='keyword_yake_patentsberta_embedding_tsne_x', \n",
    "    y='keyword_yake_patentsberta_embedding_tsne_y', \n",
    "    color='patentsberta_cluster', \n",
    "    hover_data=['patentsberta_cluster', 'keyword_yake'], \n",
    "    title='Cleantech Yake Keywords - HDBSCAN, TSNE, PatentSBERTa',\n",
    "    height=800,  # Adjust as needed\n",
    "    width=800    # Adjust as needed\n",
    ")\n",
    "# Display the figures\n",
    "fig_patentsberta.show()\n",
    "\n",
    "# fig_patentsberta.write_html('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings_tsne.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe of keywords and their corresponding clusters, where each row is a cluster with list of keywords\n",
    "df_keywords_clusters_patentsberta = df_keywords_list_unique_embeddings_hdbscan.groupby('patentsberta_cluster')['keyword_yake'].apply(list).reset_index(name='keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text-generation pipeline with Flan-T5-large\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-large', device=device)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_name(keywords):\n",
    "    # Ensure the keywords are in a list format\n",
    "    keywords = keywords.split(', ') if isinstance(keywords, str) else keywords\n",
    "    # Select only the first 1500 keywords from the list\n",
    "    selected_keywords = keywords[:2000]\n",
    "    # Join the selected keywords into a string format\n",
    "    keywords_str = ', '.join(selected_keywords)\n",
    "    # Create a prompt from the selected keywords\n",
    "    # prompt = f\"Based on the following keywords, come up with a specific, precise and short topic name: {keywords_str}\"\n",
    "    prompt = f\"Generate a concise and descriptive common theme or category for a cluster containing the following keywords: {keywords_str}.\" # The name should be in title case and should not exceed three words.\"\n",
    "    # Doesn't work at all -> only focuses on electric vehicle innovation# prompt = f\"Given the keywords: {keywords_str}, provide a succinct cluster name similar to how 'Electric Vehicle Innovation' represents keywords like 'battery technology, electric motor, charging infrastructure'.\"\n",
    "    # prompt = f\"Identify a common theme or category for the following keywords: {keywords_str}. Provide a concise, descriptive name for this theme or category.\"\n",
    "    # prompt = f\"The keywords {keywords_str} all belong to the category: _____\"\n",
    "    # Generate a response using the GPT-3 model\n",
    "    response = generator(prompt, max_length=10, do_sample=True, temperature=0.8)[0]['generated_text']\n",
    "    # Extract the cluster name from the response\n",
    "    cluster_name = response\n",
    "    return cluster_name\n",
    "\n",
    "# Apply the function to the 'keywords' column to generate cluster names\n",
    "df_keywords_clusters_patentsberta['cluster_name'] = df_keywords_clusters_patentsberta['keywords'].progress_apply(generate_cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_keywords_clusters_patentsberta with df_keywords_list_unique_pruned_embeddings_hdbscan on 'patentsberta_cluster'\n",
    "df_keywords_list_unique_embeddings_cluster = pd.merge(df_keywords_list_unique_embeddings_hdbscan, df_keywords_clusters_patentsberta[['patentsberta_cluster', 'cluster_name']], on='patentsberta_cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig_patentsberta_cluster = px.scatter(\n",
    "    df_keywords_list_unique_embeddings_cluster, \n",
    "    x='keyword_yake_patentsberta_embedding_tsne_x', \n",
    "    y='keyword_yake_patentsberta_embedding_tsne_y', \n",
    "    color='cluster_name', \n",
    "    hover_data=['patentsberta_cluster', 'keyword_yake'], \n",
    "    title='Cleantech Yake Keywords - HDBSCAN, TSNE, PatentSBERTa',\n",
    "    height=800,  # Adjust as needed\n",
    "    width=1200    # Adjust as needed\n",
    ")\n",
    "\n",
    "# Display the figures\n",
    "fig_patentsberta_cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange column order\n",
    "df_keywords_clusters_patentsberta = df_keywords_clusters_patentsberta[['patentsberta_cluster', 'cluster_name', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_clusters_patentsberta.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_clusters_patentsberta.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings_cluster.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings_tsne_hdbscan_cluster.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
