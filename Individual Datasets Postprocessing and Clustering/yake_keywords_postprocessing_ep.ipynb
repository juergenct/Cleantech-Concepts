{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_yake_claims = pd.read_json('/mnt/hdd01/PATSTAT Working Directory/PATSTAT/cleantech_epo_text_data_pivot_cleaned_yake.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yake_claims.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182369it [00:25, 7142.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over rows in dataframe\n",
    "for index, row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Cast row 'keywords_yake_claims' column to string\n",
    "    row['keywords_yake_claims'] = str(row['keywords_yake_claims']).lower()\n",
    "    # Check if keywords_yake_claims column starts with \"[[[\" and ends with \"]]]\"\"\n",
    "    if row['keywords_yake_claims'].startswith('[[[') and row['keywords_yake_claims'].endswith(']]]'):\n",
    "        # Remove first \"[\" and last \"]\" from keywords_yake_claims column\n",
    "        row['keywords_yake_claims'] = row['keywords_yake_claims'][1:-1]\n",
    "    # Cast row 'keywords_yake_claims' column to list\n",
    "    row['keywords_yake_claims'] = ast.literal_eval(row['keywords_yake_claims'])\n",
    "    # Assign modified 'keywords_yake_claims' list to temporary variable\n",
    "    keywords_yake_temp = row['keywords_yake_claims']\n",
    "    # Assign temporary variable to 'keywords_yake_claims' column\n",
    "    df_yake_claims.at[index, 'keywords_yake_claims'] = keywords_yake_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182369it [00:44, 4095.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df_yake_claims['keywords_yake_claims_exploded'] = ''\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Check if 'keywords_yake_claims' column is not a list\n",
    "    if not isinstance(row['keywords_yake_claims'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake_claims' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_claims']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake_claims' column and append to 'keywords_yake_claims_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_claims']:\n",
    "            df_yake_claims.at[index, 'keywords_yake_claims_exploded'] += keyword[0] + ', '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'keywords_yake_claims_exploded' column to list\n",
    "df_yake_claims['keywords_yake_claims_exploded'] = df_yake_claims['keywords_yake_claims_exploded'].str.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182369/182369 [00:00<00:00, 1229970.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Delete last item in 'keywords_yake_claims_exploded' cells if empty string\n",
    "df_yake_claims['keywords_yake_claims_exploded'] = df_yake_claims['keywords_yake_claims_exploded'].progress_apply(lambda x: x[:-1] if x and x[-1] == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182369it [00:06, 29717.08it/s]\n"
     ]
    }
   ],
   "source": [
    "keywords_list = []\n",
    "min_yake_conf = 0.05\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    # Check if 'keywords_yake_claims' column is not a list\n",
    "    if not isinstance(row['keywords_yake_claims'], list):\n",
    "        continue\n",
    "    # Check if 'keywords_yake_claims' column is an empty list or contains only empty lists\n",
    "    if not any(keyword for keyword in row['keywords_yake_claims']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake_claims' column and append to 'keywords_yake_claims_exploded' column\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_claims']:\n",
    "            if keyword[1] <= min_yake_conf:\n",
    "                keywords_list.append(keyword[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider alphanumeric characters in keywords\n",
    "keywords_list = [\n",
    "    keyword for keyword in keywords_list \n",
    "    if all(word.isalnum() for word in keyword.split())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique list of keywords\n",
    "keywords_list_unique = list(set(keywords_list))\n",
    "df_keywords_list_unique = pd.DataFrame(keywords_list_unique, columns=['keyword_yake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182369/182369 [00:00<00:00, 1561469.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Count absolute frequency of each keyword\n",
    "from collections import Counter\n",
    "\n",
    "# Step 1: Flatten the list of keywords into a single list\n",
    "flattened_keywords = [keyword for keyword_list in tqdm(df_yake_claims['keywords_yake_claims_exploded']) for keyword in keyword_list]\n",
    "\n",
    "# Step 2: Use collections.Counter to count the occurrences of each keyword\n",
    "keyword_counts = Counter(flattened_keywords)\n",
    "\n",
    "# Step 3: Convert the keyword counts to a DataFrame\n",
    "count_df = pd.DataFrame(keyword_counts.items(), columns=['keyword_yake', 'abs_frequency'])\n",
    "\n",
    "# Step 4: Merge the count_df with df_keywords_list_unique on the 'keyword_yake' column\n",
    "df_keywords_list_unique = df_keywords_list_unique.merge(count_df, on='keyword_yake', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAYBE I NEED TO REDO THIS WITH FULL TEXT CLAIMS INSTEAD OF JUST THE YAKE KEYWORDS\n",
    "\n",
    "# Calculate document frequency by dividing absolute frequency by len of df_yake_claims\n",
    "# df_keywords_list_unique['doc_frequency'] = df_keywords_list_unique['abs_frequency'] / len(df_yake_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune dataframe by document frequency and absolute frequency\n",
    "min_abs_frequency = 5\n",
    "max_abs_frequency = 1000\n",
    "# max_doc_frequency = 0.3\n",
    "\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique[(df_keywords_list_unique['abs_frequency'] >= min_abs_frequency) & (df_keywords_list_unique['abs_frequency'] <= max_abs_frequency)]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only consider n-grams with n >= 2 - NOT CONSIDERED FOR NOW\n",
    "# min_ngram_length = 2\n",
    "\n",
    "# # Generate copy of df_keywords_list_unique_pruned\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# def filter_keywords(keyword_string):\n",
    "#     # Split the keyword string into individual keywords\n",
    "#     keywords = keyword_string.split(', ')\n",
    "#     # Filter out keywords with fewer than 2 words\n",
    "#     filtered_keywords = [keyword for keyword in keywords if len(keyword.split()) >= min_ngram_length]\n",
    "#     # Join the filtered keywords back into a string\n",
    "#     return ', '.join(filtered_keywords)\n",
    "\n",
    "# # Create a copy of df_keywords_list_unique_pruned to avoid SettingWithCopyWarning\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# # Now use .loc to modify the 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(filter_keywords)\n",
    "\n",
    "# # Delete rows with empty 'keyword_yake' column\n",
    "# df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# # Reset index\n",
    "# df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26871/26871 [00:01<00:00, 19766.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Generate copy of df_keywords_list_unique_pruned\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Delete all stopwords from 'keyword_yake' column\n",
    "df_keywords_list_unique_pruned.loc[:, 'keyword_yake'] = df_keywords_list_unique_pruned['keyword_yake'].progress_apply(\n",
    "    lambda x: '' if x in stopwords.words('english') else x\n",
    ")\n",
    "\n",
    "# Remove empty rows\n",
    "df_keywords_list_unique_pruned = df_keywords_list_unique_pruned[df_keywords_list_unique_pruned['keyword_yake'] != '']\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiesen/Documents/Cleantech_Concepts/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 26871/26871 [01:02<00:00, 427.77it/s]\n"
     ]
    }
   ],
   "source": [
    "### MAYBE I NEED TO REDO THIS PART WITH THE CLAIM TEXTS INSTEAD OF THE KEYWORDS - NOT CONSIDERED FOR NOW\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Download spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned.copy()\n",
    "\n",
    "# Perform part-of-speech tagging on the 'keyword_yake' column \n",
    "# and save the POS tags in a new column 'keyword_yake_pos'\n",
    "df_keywords_list_unique_pruned_pos['keyword_yake_pos'] = df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(\n",
    "    lambda x: [token.pos_ for token in nlp(x)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26871/26871 [00:00<00:00, 486685.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filter out all keywords that do not contain NOUN, PRON or PROPN in their POS tags\n",
    "df_keywords_list_unique_pruned_pos = df_keywords_list_unique_pruned_pos[df_keywords_list_unique_pruned_pos['keyword_yake_pos'].progress_apply(\n",
    "    lambda x: any(pos in ['NOUN', 'PRON', 'PROPN'] for pos in x)\n",
    ")]\n",
    "\n",
    "# Reset index\n",
    "df_keywords_list_unique_pruned_pos.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT CONSIDERED FOR NOW\n",
    "# import re\n",
    "\n",
    "# # Function to check if a word contains at least three consecutive capital letters (filter out all abbreviations)\n",
    "# def has_three_consecutive_caps(word):\n",
    "#     return re.search(r'[A-Z]{3,}', word) is not None\n",
    "\n",
    "# # Function to check if any word in an n-gram contains at least three consecutive capital letters\n",
    "# def check_ngram(ngram):\n",
    "#     return any(has_three_consecutive_caps(word) for word in ngram.split())\n",
    "\n",
    "# # Use the DataFrame `apply` method to filter rows\n",
    "# df_keywords_list_unique_pruned_pos_filtered = df_keywords_list_unique_pruned_pos[~df_keywords_list_unique_pruned_pos['keyword_yake'].progress_apply(check_ngram)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT CONSIDERED FOR NOW\n",
    "# # Test to filter out brown corpus words\n",
    "# from nltk.corpus import brown\n",
    "# # nltk.download('brown')\n",
    "\n",
    "# # Step 1: Pre-calculate the set of Brown corpus words\n",
    "# brown_words_set = set(brown.words())\n",
    "\n",
    "# # Function to check if any word in a keyword phrase exists in the Brown corpus\n",
    "# def check_brown_words(phrase):\n",
    "#     return any(word in brown_words_set for word in phrase.split())\n",
    "\n",
    "# # Step 2: Use progress_apply with the optimized function\n",
    "# df_keywords_list_unique_pruned_pos_filtered_brown = df_keywords_list_unique_pruned_pos_filtered[\n",
    "#     df_keywords_list_unique_pruned_pos_filtered['keyword_yake'].progress_apply(check_brown_words)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Keywords on Patent Id and CPC Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182369it [00:11, 16527.90it/s]\n"
     ]
    }
   ],
   "source": [
    "rows_list = []\n",
    "\n",
    "# Iterate over rows in dataframe\n",
    "# for index, row in tqdm(df_yake_claims_cpc.iterrows()):\n",
    "for index,row in tqdm(df_yake_claims.iterrows()):\n",
    "    if row['keywords_yake_claims_exploded'] == []:\n",
    "        continue\n",
    "    if not any(keyword for keyword in row['keywords_yake_claims_exploded']):\n",
    "        continue\n",
    "    # Iterate over keywords in 'keywords_yake' column and append patent_id and cpc information\n",
    "    else:\n",
    "        for keyword in row['keywords_yake_claims_exploded']:\n",
    "            # EPO\n",
    "            rows_list.append({'publn_nr': row['publn_nr'],\n",
    "                              'cpc_class_symbol': row['cpc_class_symbol'],\n",
    "                              'keyword_yake_claims': keyword})\n",
    "\n",
    "df_claims_keywords = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate df_claims_keywords on 'keyword_yake_claims' column\n",
    "df_claims_keywords_agg = df_claims_keywords.groupby('keyword_yake_claims').agg(\n",
    "    publn_nr_list = pd.NamedAgg(column='publn_nr', aggfunc=list),\n",
    "    cpc_class_symbol_list = pd.NamedAgg(column='cpc_class_symbol', aggfunc=list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 602746/602746 [00:02<00:00, 287177.01it/s] \n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates from 'cpc_class_symbol_list'\n",
    "df_claims_keywords_agg['cpc_class_symbol_list'] = df_claims_keywords_agg['cpc_class_symbol_list'].progress_apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_claims_keywords_agg with df_keywords_list_unique_embeddings on 'keyword_yake_claims' column and generate new dataframe\n",
    "df_claims_keywords_list = pd.merge(df_keywords_list_unique_pruned_pos, df_claims_keywords_agg, left_on='keyword_yake', right_on='keyword_yake_claims', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f. Creating a new one with MEAN pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/thiesen/.cache/torch/sentence_transformers/climatebert_distilroberta-base-climate-f and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "No sentence-transformers model found with name /home/thiesen/.cache/torch/sentence_transformers/anferico_bert-for-patents. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model_climatebert = SentenceTransformer('climatebert/distilroberta-base-climate-f')\n",
    "model_bertforpatents = SentenceTransformer('anferico/bert-for-patents')\n",
    "model_patentsberta = SentenceTransformer('AI-Growth-Lab/PatentSBERTa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU available: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23105/23105 [02:33<00:00, 150.92it/s]\n",
      "100%|██████████| 23105/23105 [01:29<00:00, 257.39it/s]\n",
      "100%|██████████| 23105/23105 [04:55<00:00, 78.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate copy of df_claims_keywords_list\n",
    "df_keywords_list_unique_embeddings = df_claims_keywords_list.copy()\n",
    "\n",
    "# Perform sentence embedding on the 'keyword_yake' (PatentsView) or 'keywords_yake_claims' (EPO) column\n",
    "df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_patentsberta.encode(x)\n",
    ")\n",
    "\n",
    "df_keywords_list_unique_embeddings['keyword_yake_climatebert_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_climatebert.encode(x)\n",
    ")\n",
    "\n",
    "df_keywords_list_unique_embeddings['keyword_yake_bertforpatents_embedding'] = df_keywords_list_unique_embeddings['keyword_yake'].progress_apply(\n",
    "    lambda x: model_bertforpatents.encode(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_yake</th>\n",
       "      <th>abs_frequency</th>\n",
       "      <th>keyword_yake_pos</th>\n",
       "      <th>publn_nr_list</th>\n",
       "      <th>cpc_class_symbol_list</th>\n",
       "      <th>keyword_yake_patentsberta_embedding</th>\n",
       "      <th>keyword_yake_climatebert_embedding</th>\n",
       "      <th>keyword_yake_bertforpatents_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acetic anhydride</td>\n",
       "      <td>9</td>\n",
       "      <td>[ADJ, NOUN]</td>\n",
       "      <td>[380911, 381988, 810028, 1132384, 1220829, 137...</td>\n",
       "      <td>[['Y02P  20/54', 'Y02P  20/584'], ['Y02P  20/5...</td>\n",
       "      <td>[-0.061444506, -1.0947561, -0.2616166, -0.1942...</td>\n",
       "      <td>[0.0051785745, 0.21960728, -0.05591166, -0.108...</td>\n",
       "      <td>[0.4696103, 0.05110017, -0.06434366, 0.2766463...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>detectors</td>\n",
       "      <td>7</td>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>[64008, 280925, 1683698, 1842205, 2294406, 262...</td>\n",
       "      <td>[['Y02D  30/70'], ['Y02T  10/62', 'Y02T  10/70...</td>\n",
       "      <td>[-0.12924775, -0.4697809, -0.36494324, 0.32934...</td>\n",
       "      <td>[0.015357403, 0.17548731, 0.03343723, 0.090828...</td>\n",
       "      <td>[-0.40965247, 0.7753821, -1.0722224, 0.0085351...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>digital subscriber line</td>\n",
       "      <td>6</td>\n",
       "      <td>[PROPN, NOUN, NOUN]</td>\n",
       "      <td>[1189422, 1843610, 2832086, 2907270, 2907282, ...</td>\n",
       "      <td>[['Y02D  30/50'], ['Y02D  30/50', 'Y02D  30/70...</td>\n",
       "      <td>[-0.051423278, -0.27481222, -0.1730148, -0.450...</td>\n",
       "      <td>[0.014533296, -0.046291605, -0.06059263, -0.12...</td>\n",
       "      <td>[0.0066918074, 0.60324126, -0.11662749, -1.302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grain</td>\n",
       "      <td>42</td>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>[62173, 94114, 267633, 462232, 514118, 586415,...</td>\n",
       "      <td>[['Y02W  30/91'], ['Y02E  30/30'], ['Y02P  10/...</td>\n",
       "      <td>[-0.20005703, -0.2817135, -0.24730219, 0.08159...</td>\n",
       "      <td>[-0.03290707, 0.0750718, 0.060500715, 0.080330...</td>\n",
       "      <td>[-0.38387752, -0.28565928, 0.11016483, -0.9580...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>processors</td>\n",
       "      <td>27</td>\n",
       "      <td>[NOUN]</td>\n",
       "      <td>[517558, 1387273, 1653332, 1916802, 2005659, 2...</td>\n",
       "      <td>[['Y02T  50/40'], ['Y02A  90/10'], ['Y02D  10/...</td>\n",
       "      <td>[-0.027116872, -0.34918648, -0.37371352, 0.364...</td>\n",
       "      <td>[-0.014848103, 0.07489975, -0.006813975, -0.02...</td>\n",
       "      <td>[-0.19758111, -0.15376149, -0.85155207, -0.059...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              keyword_yake  abs_frequency     keyword_yake_pos  \\\n",
       "0         acetic anhydride              9          [ADJ, NOUN]   \n",
       "1                detectors              7               [NOUN]   \n",
       "2  digital subscriber line              6  [PROPN, NOUN, NOUN]   \n",
       "3                    grain             42               [NOUN]   \n",
       "4               processors             27               [NOUN]   \n",
       "\n",
       "                                       publn_nr_list  \\\n",
       "0  [380911, 381988, 810028, 1132384, 1220829, 137...   \n",
       "1  [64008, 280925, 1683698, 1842205, 2294406, 262...   \n",
       "2  [1189422, 1843610, 2832086, 2907270, 2907282, ...   \n",
       "3  [62173, 94114, 267633, 462232, 514118, 586415,...   \n",
       "4  [517558, 1387273, 1653332, 1916802, 2005659, 2...   \n",
       "\n",
       "                               cpc_class_symbol_list  \\\n",
       "0  [['Y02P  20/54', 'Y02P  20/584'], ['Y02P  20/5...   \n",
       "1  [['Y02D  30/70'], ['Y02T  10/62', 'Y02T  10/70...   \n",
       "2  [['Y02D  30/50'], ['Y02D  30/50', 'Y02D  30/70...   \n",
       "3  [['Y02W  30/91'], ['Y02E  30/30'], ['Y02P  10/...   \n",
       "4  [['Y02T  50/40'], ['Y02A  90/10'], ['Y02D  10/...   \n",
       "\n",
       "                 keyword_yake_patentsberta_embedding  \\\n",
       "0  [-0.061444506, -1.0947561, -0.2616166, -0.1942...   \n",
       "1  [-0.12924775, -0.4697809, -0.36494324, 0.32934...   \n",
       "2  [-0.051423278, -0.27481222, -0.1730148, -0.450...   \n",
       "3  [-0.20005703, -0.2817135, -0.24730219, 0.08159...   \n",
       "4  [-0.027116872, -0.34918648, -0.37371352, 0.364...   \n",
       "\n",
       "                  keyword_yake_climatebert_embedding  \\\n",
       "0  [0.0051785745, 0.21960728, -0.05591166, -0.108...   \n",
       "1  [0.015357403, 0.17548731, 0.03343723, 0.090828...   \n",
       "2  [0.014533296, -0.046291605, -0.06059263, -0.12...   \n",
       "3  [-0.03290707, 0.0750718, 0.060500715, 0.080330...   \n",
       "4  [-0.014848103, 0.07489975, -0.006813975, -0.02...   \n",
       "\n",
       "               keyword_yake_bertforpatents_embedding  \n",
       "0  [0.4696103, 0.05110017, -0.06434366, 0.2766463...  \n",
       "1  [-0.40965247, 0.7753821, -1.0722224, 0.0085351...  \n",
       "2  [0.0066918074, 0.60324126, -0.11662749, -1.302...  \n",
       "3  [-0.38387752, -0.28565928, 0.11016483, -0.9580...  \n",
       "4  [-0.19758111, -0.15376149, -0.85155207, -0.059...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keywords_list_unique_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to json - Make sure to use correct directory (PatentsView vs. EPO)\n",
    "df_keywords_list_unique_embeddings.to_json('/mnt/hdd01/PATSTAT Working Directory/PATSTAT/cleantech_epo_text_data_pivot_cleaned_yake_embeddings_processed.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings.to_json('/mnt/hdd01/patentsview/Similarity Search - CPC Classification and Claims/df_keywords_y02_ep_embeddings_processed.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data if necessary\n",
    "df_keywords_list_unique_embeddings = pd.read_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "umap_dim = 2\n",
    "reducer = umap.UMAP(random_state=42, n_components=umap_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all the embeddings into a 2D array\n",
    "embeddings_patentsberta = np.vstack(df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'].values)\n",
    "\n",
    "# Perform UMAP dimensionality reduction\n",
    "umap_embeddings_patentsberta = reducer.fit_transform(embeddings_patentsberta)\n",
    "\n",
    "# Assign the reduced dimension embeddings back to new DataFrame columns\n",
    "df_keywords_list_unique_embeddings_umap['keyword_yake_patentsberta_embedding_umap'] = list(umap_embeddings_patentsberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openTSNE import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/test split\n",
    "patentsberta_x_train, patentsberta_x_test = train_test_split(df_keywords_list_unique_embeddings['keyword_yake_patentsberta_embedding'].tolist(), test_size=0.2, random_state=42)\n",
    "patentsberta_x_train_np = np.array(patentsberta_x_train)\n",
    "patentsberta_x_test_np = np.array(patentsberta_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    random_state=42,\n",
    "    n_iter=1000,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-SNE on training data\n",
    "patentsberta_embedding_train = tsne.fit(patentsberta_x_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patentsberta_embedding_test = patentsberta_embedding_train.transform(patentsberta_x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings_tsne = df_keywords_list_unique_embeddings.copy()\n",
    "\n",
    "patentsberta_embedding = np.concatenate((patentsberta_embedding_train, patentsberta_embedding_test), axis=0)\n",
    "df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne'] = patentsberta_embedding.tolist()\n",
    "df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne_x'] = df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne'].apply(lambda x: x[0])\n",
    "df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne_y'] = df_keywords_list_unique_embeddings_tsne['keyword_yake_patentsberta_embedding_tsne'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import HDBSCAN\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you're working on a new DataFrame, not a slice of an old one\n",
    "df_keywords_list_unique_embeddings_hdbscan = df_keywords_list_unique_embeddings_tsne.copy()\n",
    "\n",
    "# Perform HDBSCAN clustering on the UMAP coordinates\n",
    "clusterer_patentsberta = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=1).fit(df_keywords_list_unique_embeddings_hdbscan[['keyword_yake_patentsberta_embedding_tsne_x', 'keyword_yake_patentsberta_embedding_tsne_y']])\n",
    "\n",
    "# Assign the cluster labels back to the DataFrame\n",
    "df_keywords_list_unique_embeddings_hdbscan['patentsberta_cluster'] = clusterer_patentsberta.labels_\n",
    "\n",
    "# Erase all rows with cluster -1\n",
    "df_keywords_list_unique_embeddings_hdbscan = df_keywords_list_unique_embeddings_hdbscan[df_keywords_list_unique_embeddings_hdbscan['patentsberta_cluster'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings_hdbscan.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings_tsne_hdbscan.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP embeddings in three figures, color coded by cluster\n",
    "fig_patentsberta = px.scatter(\n",
    "    df_keywords_list_unique_embeddings_hdbscan, \n",
    "    x='keyword_yake_patentsberta_embedding_tsne_x', \n",
    "    y='keyword_yake_patentsberta_embedding_tsne_y', \n",
    "    color='patentsberta_cluster', \n",
    "    hover_data=['patentsberta_cluster', 'keyword_yake'], \n",
    "    title='Cleantech Yake Keywords - HDBSCAN, TSNE, PatentSBERTa',\n",
    "    height=800,  # Adjust as needed\n",
    "    width=800    # Adjust as needed\n",
    ")\n",
    "# Display the figures\n",
    "fig_patentsberta.show()\n",
    "\n",
    "# fig_patentsberta.write_html('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings_tsne.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe of keywords and their corresponding clusters, where each row is a cluster with list of keywords\n",
    "df_keywords_clusters_patentsberta = df_keywords_list_unique_embeddings_hdbscan.groupby('patentsberta_cluster')['keyword_yake'].apply(list).reset_index(name='keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text-generation pipeline with Flan-T5-large\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-large', device=device)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_name(keywords):\n",
    "    # Ensure the keywords are in a list format\n",
    "    keywords = keywords.split(', ') if isinstance(keywords, str) else keywords\n",
    "    # Select only the first 1500 keywords from the list\n",
    "    selected_keywords = keywords[:2000]\n",
    "    # Join the selected keywords into a string format\n",
    "    keywords_str = ', '.join(selected_keywords)\n",
    "    # Create a prompt from the selected keywords\n",
    "    # prompt = f\"Based on the following keywords, come up with a specific, precise and short topic name: {keywords_str}\"\n",
    "    prompt = f\"Generate a concise and descriptive common theme or category for a cluster containing the following keywords: {keywords_str}.\" # The name should be in title case and should not exceed three words.\"\n",
    "    # Doesn't work at all -> only focuses on electric vehicle innovation# prompt = f\"Given the keywords: {keywords_str}, provide a succinct cluster name similar to how 'Electric Vehicle Innovation' represents keywords like 'battery technology, electric motor, charging infrastructure'.\"\n",
    "    # prompt = f\"Identify a common theme or category for the following keywords: {keywords_str}. Provide a concise, descriptive name for this theme or category.\"\n",
    "    # prompt = f\"The keywords {keywords_str} all belong to the category: _____\"\n",
    "    # Generate a response using the GPT-3 model\n",
    "    response = generator(prompt, max_length=10, do_sample=True, temperature=0.8)[0]['generated_text']\n",
    "    # Extract the cluster name from the response\n",
    "    cluster_name = response\n",
    "    return cluster_name\n",
    "\n",
    "# Apply the function to the 'keywords' column to generate cluster names\n",
    "df_keywords_clusters_patentsberta['cluster_name'] = df_keywords_clusters_patentsberta['keywords'].progress_apply(generate_cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_keywords_clusters_patentsberta with df_keywords_list_unique_pruned_embeddings_hdbscan on 'patentsberta_cluster'\n",
    "df_keywords_list_unique_embeddings_cluster = pd.merge(df_keywords_list_unique_embeddings_hdbscan, df_keywords_clusters_patentsberta[['patentsberta_cluster', 'cluster_name']], on='patentsberta_cluster', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig_patentsberta_cluster = px.scatter(\n",
    "    df_keywords_list_unique_embeddings_cluster, \n",
    "    x='keyword_yake_patentsberta_embedding_tsne_x', \n",
    "    y='keyword_yake_patentsberta_embedding_tsne_y', \n",
    "    color='cluster_name', \n",
    "    hover_data=['patentsberta_cluster', 'keyword_yake'], \n",
    "    title='Cleantech Yake Keywords - HDBSCAN, TSNE, PatentSBERTa',\n",
    "    height=800,  # Adjust as needed\n",
    "    width=1200    # Adjust as needed\n",
    ")\n",
    "\n",
    "# Display the figures\n",
    "fig_patentsberta_cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange column order\n",
    "df_keywords_clusters_patentsberta = df_keywords_clusters_patentsberta[['patentsberta_cluster', 'cluster_name', 'keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_clusters_patentsberta.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_clusters_patentsberta.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords_list_unique_embeddings_cluster.to_json('/mnt/hdd01/patentsview/Patentsview - Cleantech Patents/Cleantech Concepts/Yake/g_patent_claims_cleantech_yake_keywords_embeddings_tsne_hdbscan_cluster.json', orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
